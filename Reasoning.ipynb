{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO+eFukYbmAA0QuUssZ4V8F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AfsanehHabibi/reddit-conversation-quality/blob/main/Reasoning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install anytree"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Baf9D1PyJyQy",
        "outputId": "1feb7e0f-a94d-4a09-f8eb-520e72974b27"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting anytree\n",
            "  Downloading anytree-2.12.1-py3-none-any.whl (44 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.9/44.9 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from anytree) (1.16.0)\n",
            "Installing collected packages: anytree\n",
            "Successfully installed anytree-2.12.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gH_3qPmYLjG6",
        "outputId": "6a32a3f5-a074-4e8a-b771-fd0d5151743e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_path = \"/content/drive/MyDrive/University/RedditData/\""
      ],
      "metadata": {
        "id": "X8ggZeKJLnQ3"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Reading from a file using Pickle\n",
        "with open(f\"{base_path}conversations_with_reasoning.pkl\", 'rb') as file:\n",
        "    conversations = pickle.load(file)\n",
        "\n",
        "# Print the loaded variables\n",
        "print(\"Len conversations:\", len(conversations))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "saeRLzg7Lxor",
        "outputId": "418fefb4-3463-4a25-d213-78c9d6a95f74"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Len conversations: 32990\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "u5J56ZGyiP6-"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def contains_reasoning(text):\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Check for specific part-of-speech patterns indicating reasoning\n",
        "    reasoning_patterns = [\"because\", \"since\", \"therefore\", \"due to\", \"as a result\", \"consequently\", \"thus\",\n",
        "                          \"for this reason\", \"in conclusion\", \"owing to\", \"on account of\", \"resulting in\",\n",
        "                          \"so\", \"hence\", \"in light of\", \"accordingly\", \"on the grounds that\"]\n",
        "\n",
        "    for token in doc:\n",
        "        if token.text.lower() in reasoning_patterns:\n",
        "            return True\n",
        "\n",
        "    return False"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "VERBOSE = 3000"
      ],
      "metadata": {
        "id": "IMfdi2ltoTe8"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "def write_conversations_with_reasoning():\n",
        "  with open(f\"{base_path}conversations_reasoning.pkl\", 'wb') as file:\n",
        "    pickle.dump(conversations, file)\n",
        "  print(\"Variables written to the file.\")"
      ],
      "metadata": {
        "id": "xOAANFTXTOi1"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_comments(conversations, num_break_points):\n",
        "  flat_comments_list = []\n",
        "  break_points_index = []\n",
        "  counter = 0\n",
        "  break_length = int(len(conversations)/num_break_points)\n",
        "  print(\"break len \", break_length)\n",
        "  for id in conversations:\n",
        "    if counter % break_length == 0:\n",
        "      break_points_index.append(len(flat_comments_list))\n",
        "    counter += 1\n",
        "    comments = conversations[id]['comments']\n",
        "    for comment in comments:\n",
        "      flat_comments_list.append(comment)\n",
        "\n",
        "  return flat_comments_list, break_points_index"
      ],
      "metadata": {
        "id": "RwKqR4ydx3ko"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "def write_reasoning_dic(reasoning_dic, file_path):\n",
        "  with open(file_path, 'wb') as file:\n",
        "      pickle.dump(reasoning_dic, file)\n",
        "  print(\"Variables written to the file.\")"
      ],
      "metadata": {
        "id": "pwoEcV_Sx-Gp"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "def merge_conversations_with_reasoning(conversations, file_pathes, method):\n",
        "  reasoning_dic = dict()\n",
        "  for file_path in file_pathes:\n",
        "    with open(file_path, 'rb') as file:\n",
        "      reasoning_part = pickle.load(file)\n",
        "      reasoning_dic.update(reasoning_part)\n",
        "\n",
        "  for id in conversations:\n",
        "    comments = conversations[id]['comments']\n",
        "    for comment in comments:\n",
        "      if not(comment['body'] == '[deleted]' or comment['body'] == '[removed]'):\n",
        "        if method != \"flasher\":\n",
        "          comment['has_reasoning'+method] = reasoning_dic[comment['id']]\n",
        "        else:\n",
        "          comment['has_reasoning'] = reasoning_dic[comment['id']]\n",
        "  with open(f\"{base_path}conversations_with_reasoning.pkl\", 'wb') as file:\n",
        "    pickle.dump(conversations, file)\n",
        "  print(\"Variables written to the file.\")"
      ],
      "metadata": {
        "id": "9U4AeeohyBdw"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_reasoning_for_corpus(corpus, method):\n",
        "  reasoning_dic = dict()\n",
        "  for comment in corpus:\n",
        "      if not(comment['body'] == '[deleted]' or comment['body'] == '[removed]'):\n",
        "          if method == \"flasher\":\n",
        "            has_reasoning = contains_reasoning(comment['body'])\n",
        "          elif \"one-shot\":\n",
        "            has_reasoning = reasoning_based_on_one_shot_classification(comment['body'])\n",
        "          reasoning_dic[comment['id']] = has_reasoning\n",
        "  return reasoning_dic"
      ],
      "metadata": {
        "id": "YTbNW3XnzU8k"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def calculate_reasoning(conversations, num_of_parts, rewrite, method):\n",
        "  flat_comments, break_points_index = extract_comments(conversations, num_of_parts)\n",
        "  print(\"len \", len(break_points_index))\n",
        "  break_points_index.append(len(flat_comments))\n",
        "  file_pathes = []\n",
        "  for i in range(1, len(break_points_index)):\n",
        "    print(\"range \", break_points_index[i-1], break_points_index[i])\n",
        "    file_path = f\"{base_path}comments_reasoning_{break_points_index[i-1]}_{break_points_index[i]}.pkl\"\n",
        "    if method != \"flasher\":\n",
        "      file_path = f\"{base_path}comments_{method}_reasoning_{break_points_index[i-1]}_{break_points_index[i]}.pkl\"\n",
        "    file_pathes.append(file_path)\n",
        "    print(\"path \", file_path)\n",
        "    if rewrite or not os.path.exists(file_path):\n",
        "      print(\"do\")\n",
        "      corpus = flat_comments[break_points_index[i-1]:break_points_index[i]]\n",
        "      reasoning_dic = calculate_reasoning_for_corpus(corpus, method)\n",
        "      write_reasoning_dic(reasoning_dic, file_path)\n",
        "  merge_conversations_with_reasoning(conversations, file_pathes, method)"
      ],
      "metadata": {
        "id": "q9xvDYzPyFzU"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calculate_reasoning(conversations, 10, False, \"flasher\")\n",
        "write_conversations_with_reasoning()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rB50wkEyJX5",
        "outputId": "d847ebdf-9fdf-4473-925f-0b47f5cddf48"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "break len  3299\n",
            "len  10\n",
            "range  0 53506\n",
            "path  /content/drive/MyDrive/University/RedditData/comments_reasoning_0_53506.pkl\n",
            "range  53506 116967\n",
            "path  /content/drive/MyDrive/University/RedditData/comments_reasoning_53506_116967.pkl\n",
            "range  116967 178801\n",
            "path  /content/drive/MyDrive/University/RedditData/comments_reasoning_116967_178801.pkl\n",
            "range  178801 234319\n",
            "path  /content/drive/MyDrive/University/RedditData/comments_reasoning_178801_234319.pkl\n",
            "range  234319 300115\n",
            "path  /content/drive/MyDrive/University/RedditData/comments_reasoning_234319_300115.pkl\n",
            "range  300115 357006\n",
            "path  /content/drive/MyDrive/University/RedditData/comments_reasoning_300115_357006.pkl\n",
            "range  357006 424960\n",
            "path  /content/drive/MyDrive/University/RedditData/comments_reasoning_357006_424960.pkl\n",
            "range  424960 478226\n",
            "path  /content/drive/MyDrive/University/RedditData/comments_reasoning_424960_478226.pkl\n",
            "range  478226 533555\n",
            "path  /content/drive/MyDrive/University/RedditData/comments_reasoning_478226_533555.pkl\n",
            "range  533555 589734\n",
            "path  /content/drive/MyDrive/University/RedditData/comments_reasoning_533555_589734.pkl\n",
            "Variables written to the file.\n",
            "Variables written to the file.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "calculate_reasoning(conversations, 10, False, )\n",
        "write_conversations_with_reasoning(\"one-shot\")"
      ],
      "metadata": {
        "id": "GoS8_GQ99ZBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# This model is a `zero-shot-classification` model.\n",
        "# It will classify text, except you are free to choose any label you might imagine\n",
        "classifier = pipeline(model=\"facebook/bart-large-mnli\")\n",
        "classifier(\n",
        "    \"I have a problem with my iphone that needs to be resolved asap!!\",\n",
        "    candidate_labels=[\"urgent\", \"not urgent\", \"phone\", \"tablet\", \"computer\"],\n",
        ")"
      ],
      "metadata": {
        "id": "lRt0l2tUUMeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "def contains_reasoning_pipeline(text):\n",
        "    # Load the BERT model for text classification\n",
        "    classifier = pipeline('text-classification')\n",
        "\n",
        "    # Define a prompt to classify\n",
        "    prompt = f\"Is the following text reasoning about something? {text}\"\n",
        "\n",
        "    # Use the BERT model to classify the prompt\n",
        "    print(classifier(prompt))\n",
        "\n",
        "    # Check if the predicted label is positive (indicating reasoning)\n",
        "    return False\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gb6mA16VsPKQ",
        "outputId": "fbabb83d-cc75-4dcf-8d84-ce7551d55bc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'label': 'NEGATIVE', 'score': 0.9996144771575928}]\n",
            "The text does not contain reasoning.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "question_answerer = pipeline(task=\"question-answering\")\n",
        "preds = question_answerer(\n",
        "    question=\"What is the reason?\",\n",
        "    context=\"because of everything you said earlier.\",\n",
        ")\n",
        "print(\n",
        "    f\"score: {round(preds['score'], 4)}, start: {preds['start']}, end: {preds['end']}, answer: {preds['answer']}\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5vuQo49uvMm",
        "outputId": "e90f843b-5d48-4acf-d17f-1ee1c7e669a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "score: 0.2928, start: 0, end: 38, answer: because of everything you said earlier\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
        "texts = [\n",
        "    \"Djokovic has qualified for his sixth French Open final!\",\n",
        "    \"On March 23, 2010, President Obama signed the Affordable Care Act into law, putting in place comprehensive reforms that improve access to affordable health coverage for everyone and protect consumers from abusive insurance company practices\",\n",
        "    \"The goal was to show that scientists from various disciplines, diverse cultures and countries at different stages of development could find common ground about the conditions for triggering climate action in the current economic context\"\n",
        "]\n",
        "\n",
        "labels = [\"Sport\", \"Politics\", \"Environment\"]\n",
        "\n",
        "for text in texts:\n",
        "    result = classifier(text, labels, multi_label=False)\n",
        "    print(f\"Text: {text}\")\n",
        "    print(\"Predicted Labels:\")\n",
        "    for label, score in zip(result[\"labels\"], result[\"scores\"]):\n",
        "        print(f\"- {label}: {score}\")\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydB8otNmhIpb",
        "outputId": "6347ad8b-4e39-4a82-a0dd-97bcb7c00687"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: Djokovic has qualified for his sixth French Open final!\n",
            "Predicted Labels:\n",
            "- Sport: 0.858054518699646\n",
            "- Environment: 0.1092466339468956\n",
            "- Politics: 0.032698873430490494\n",
            "\n",
            "Text: On March 23, 2010, President Obama signed the Affordable Care Act into law, putting in place comprehensive reforms that improve access to affordable health coverage for everyone and protect consumers from abusive insurance company practices\n",
            "Predicted Labels:\n",
            "- Politics: 0.5848667025566101\n",
            "- Sport: 0.21482090651988983\n",
            "- Environment: 0.2003123015165329\n",
            "\n",
            "Text: The goal was to show that scientists from various disciplines, diverse cultures and countries at different stages of development could find common ground about the conditions for triggering climate action in the current economic context\n",
            "Predicted Labels:\n",
            "- Environment: 0.8344601988792419\n",
            "- Sport: 0.08409008383750916\n",
            "- Politics: 0.08144965022802353\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts = [\n",
        "    \"Please stop being a jerk, this was the first that came to me\",\n",
        "    \"Please stop being a weirdo, my comment was 20 days ago\",\n",
        "    \"I replaced two living trees with glowing mushroom trees - all the way to the bottom! Extra pictures show before, map, and bloodmoon.\",\n",
        "    \"Yeah, I'm doing the upgrade refund also, but I mean the q will increase overtime which reduce the income, thats why I had to reset it\",\n",
        "    \"Average day on r\\\\/memes\",\n",
        "    \"The father confirmed the pictures so it should be true.\",\n",
        "    \"You're in South Africa, the average IQ is 70. Of course there are no signs of intelligent life\",\n",
        "    \"They should reaaally take out the 'Book 1'\"\n",
        "]\n",
        "\n",
        "#labels = [\"HaveReason\", \"DoNotHaveReason\"]\n",
        "#labels = [\"HaveReasoning\", \"DoNotHaveReasoning\"]\n",
        "labels = ['Reasoning', \"WithoutReasoning\"]\n",
        "\n",
        "for text in texts:\n",
        "    result = classifier(text, labels, multi_label=False)\n",
        "    print(f\"Text: {text}\")\n",
        "    print(\"Predicted Labels:\")\n",
        "    for label, score in zip(result[\"labels\"], result[\"scores\"]):\n",
        "        print(f\"- {label}: {score}\")\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y730ANSUhtX5",
        "outputId": "c8c24865-9bc3-473f-b007-a98d9afc75dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: Please stop being a jerk, this was the first that came to me\n",
            "Predicted Labels:\n",
            "- WithoutReasoning: 0.8761652112007141\n",
            "- Reasoning: 0.12383478134870529\n",
            "\n",
            "Text: Please stop being a weirdo, my comment was 20 days ago\n",
            "Predicted Labels:\n",
            "- WithoutReasoning: 0.8846184015274048\n",
            "- Reasoning: 0.1153816282749176\n",
            "\n",
            "Text: I replaced two living trees with glowing mushroom trees - all the way to the bottom! Extra pictures show before, map, and bloodmoon.\n",
            "Predicted Labels:\n",
            "- WithoutReasoning: 0.623677134513855\n",
            "- Reasoning: 0.376322865486145\n",
            "\n",
            "Text: Yeah, I'm doing the upgrade refund also, but I mean the q will increase overtime which reduce the income, thats why I had to reset it\n",
            "Predicted Labels:\n",
            "- WithoutReasoning: 0.5409942269325256\n",
            "- Reasoning: 0.45900580286979675\n",
            "\n",
            "Text: Average day on r\\/memes\n",
            "Predicted Labels:\n",
            "- WithoutReasoning: 0.7949472069740295\n",
            "- Reasoning: 0.20505277812480927\n",
            "\n",
            "Text: The father confirmed the pictures so it should be true.\n",
            "Predicted Labels:\n",
            "- Reasoning: 0.7656868100166321\n",
            "- WithoutReasoning: 0.2343132048845291\n",
            "\n",
            "Text: You're in South Africa, the average IQ is 70. Of course there are no signs of intelligent life\n",
            "Predicted Labels:\n",
            "- WithoutReasoning: 0.7622557878494263\n",
            "- Reasoning: 0.23774422705173492\n",
            "\n",
            "Text: They should reaaally take out the 'Book 1'\n",
            "Predicted Labels:\n",
            "- WithoutReasoning: 0.7044756412506104\n",
            "- Reasoning: 0.29552432894706726\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20hAZn3stePS",
        "outputId": "3bdeac38-6d05-4c34-d4c9-0d6baabbed79"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentence-transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/86.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m81.9/86.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.35.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.16.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (3.8.1)\n",
            "Collecting sentencepiece (from sentence-transformers)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.20.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.4.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125923 sha256=c0d50f63f7ae58591edd1bcfa0333c69fbff74905528cebc24aaffaba5f58b70\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: sentencepiece, sentence-transformers\n",
            "Successfully installed sentence-transformers-2.2.2 sentencepiece-0.1.99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# Load the pre-trained model\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Define two example sentences\n",
        "sentence1 = \"That is a happy person.\"\n",
        "sentence2 = \"That is a very happy person.\"\n",
        "\n",
        "# Encode the sentences into embeddings\n",
        "embedding1 = model.encode(sentence1, convert_to_tensor=True)\n",
        "embedding2 = model.encode(sentence2, convert_to_tensor=True)\n",
        "\n",
        "# Calculate cosine similarity between the embeddings\n",
        "cosine_similarity = util.pytorch_cos_sim(embedding1, embedding2)\n",
        "\n",
        "# Print the similarity score\n",
        "print(\"Cosine Similarity:\", cosine_similarity.item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSxJuls5tYyT",
        "outputId": "74093ed1-9cb4-42b2-8f80-aafd3415304f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Similarity: 0.9474425315856934\n"
          ]
        }
      ]
    }
  ]
}