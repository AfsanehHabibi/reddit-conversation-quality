{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AfsanehHabibi/reddit-conversation-quality/blob/main/Relatedness.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install anytree"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hIJLKLBxfpqt",
        "outputId": "b8bbf5f7-39b8-44a3-dc3e-c020f91eb45d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting anytree\n",
            "  Downloading anytree-2.12.1-py3-none-any.whl (44 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.9/44.9 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from anytree) (1.16.0)\n",
            "Installing collected packages: anytree\n",
            "Successfully installed anytree-2.12.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMN067E-UEM-",
        "outputId": "5c54f770-812c-44e4-b735-31dd12205fe7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mv41xLXKWkwr",
        "outputId": "9547fa68-fbdf-492b-b314-2f755941251d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_path = \"/content/drive/MyDrive/University/RedditData/\""
      ],
      "metadata": {
        "id": "awKAZ4mHYPEj"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "YP5Oppx3W8gY"
      },
      "outputs": [],
      "source": [
        "#%run '/content/drive/MyDrive/ColabNotebooks/Main.ipynb'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import LongformerTokenizer, LongformerModel\n",
        "import torch\n",
        "\n",
        "def calculate_relatedness_with_longgormer():\n",
        "  # Initialize the Longformer tokenizer and model\n",
        "  model_name = \"allenai/longformer-base-4096\"\n",
        "  tokenizer = LongformerTokenizer.from_pretrained(model_name)\n",
        "  model = LongformerModel.from_pretrained(model_name)\n",
        "  # List to store encoded data\n",
        "  encoded_data = []\n",
        "\n",
        "  # Your dataset of text, e.g., comments, replies, submissions\n",
        "  text_data = []\n",
        "  for comment in flat_comments_list:\n",
        "      text_data.append(comment['body'])\n",
        "\n",
        "  for comment in flat_comments_list:\n",
        "      # Tokenize and encode the text\n",
        "      encoded = tokenizer.encode(comment['body'], add_special_tokens=True, max_length=4096, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "      # Append the encoded text to the list\n",
        "      # encoded_data.append(input_ids)\n",
        "      comment['body_encoded'] = encoded\n",
        "  # inputs_embeddings = []\n",
        "  counter = 0\n",
        "  for comment in flat_comments_list:\n",
        "    # Convert input_ids to a PyTorch tensor\n",
        "    # Modify this line to create a tensor using .clone().detach()\n",
        "    data = comment['body_encoded'].clone().detach()\n",
        "    # Forward pass through BERT\n",
        "    with torch.no_grad():\n",
        "        outputs = model(data) #.unsqueeze(0))  Add batch dimension (1) for single input\n",
        "\n",
        "    # Extract embeddings for the [CLS] token (the first token in the sequence)\n",
        "    comment['body_embeddings'] = outputs[0][:, 0, :].numpy()\n",
        "    counter += 1\n",
        "    print(counter)\n",
        "\n",
        "  # Initialize the Longformer tokenizer and model\n",
        "  model_name = \"allenai/longformer-base-4096\"\n",
        "  tokenizer = LongformerTokenizer.from_pretrained(model_name)\n",
        "  model = LongformerModel.from_pretrained(model_name)\n",
        "\n",
        "  # List to store encoded data and corresponding comments\n",
        "  encoded_data = []\n",
        "  comment_list = []\n",
        "  batch_size = 16  # You can adjust this based on your hardware capabilities\n",
        "\n",
        "  counter = 0\n",
        "  # Process comments in batches\n",
        "  for i in range(0, len(flat_comments_list), batch_size):\n",
        "      print(counter)\n",
        "      counter+= 1\n",
        "      batch_comments = flat_comments_list[i:i + batch_size]\n",
        "      batch_texts = [comment['body'] for comment in batch_comments]\n",
        "\n",
        "      # Tokenize and encode the text for the batch\n",
        "      batch_encoded = tokenizer(batch_texts, add_special_tokens=True, max_length=4096, truncation=True, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "      with torch.no_grad():\n",
        "          # Forward pass through BERT for the batch\n",
        "          outputs = model(batch_encoded['input_ids'])\n",
        "\n",
        "      # Extract embeddings for the [CLS] token (the first token in the sequence) for the batch\n",
        "      batch_embeddings = outputs[0][:, 0, :].numpy()\n",
        "\n",
        "      # Store the encoded data and corresponding comments\n",
        "      encoded_data.extend(batch_embeddings)\n",
        "      comment_list.extend(batch_comments)\n",
        "\n",
        "  # Now, 'encoded_data' contains the embeddings, and 'comment_list' contains the corresponding comments"
      ],
      "metadata": {
        "id": "wY5ixUb2YUhc"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KwyMCKNQlBXo",
        "outputId": "2d84d40b-e2f1-49f3-c3fd-849212e8327b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement datsets (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for datsets\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.99\n"
          ]
        }
      ],
      "source": [
        "!pip install datsets transformers[sentencepiece]\n",
        "\n",
        "!pip install sentencepiece\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "t4rn67-wkkpj"
      },
      "outputs": [],
      "source": [
        "from transformers import T5Tokenizer, T5Model\n",
        "import torch\n",
        "\n",
        "def google_longformer():\n",
        "  # Initialize the LongT5 tokenizer and model\n",
        "  model_name = \"google/long-t5-tglobal-base\"\n",
        "  tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "  model = T5Model.from_pretrained(model_name)\n",
        "\n",
        "  # List to store encoded data and corresponding comments\n",
        "  encoded_data = []\n",
        "\n",
        "  for comment in flat_comments_list:\n",
        "      # Tokenize and encode the text\n",
        "      encoded = tokenizer.encode(comment['body'], add_special_tokens=True, max_length=4096, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "      # Forward pass through the LongT5 model\n",
        "      with torch.no_grad():\n",
        "          outputs = model(input_ids=encoded)\n",
        "\n",
        "      # Extract embeddings for the [CLS] token (the first token in the sequence)\n",
        "      comment['body_embeddings'] = outputs.last_hidden_state[:, 0, :].numpy()\n",
        "\n",
        "      # Append the encoded data\n",
        "      encoded_data.append(comment)\n",
        "\n",
        "  # Now, 'encoded_data' contains the embeddings for your comments\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "9SuN3iIWurV5"
      },
      "outputs": [],
      "source": [
        "def jaccard_similarity(comment1, comment2):\n",
        "    set1 = set(comment1.lower().split())  # Convert to lowercase and split into words\n",
        "    set2 = set(comment2.lower().split())\n",
        "    intersection = len(set1.intersection(set2))\n",
        "    union = len(set1) + len(set2) - intersection\n",
        "    return intersection / union\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "z70yvX8brEL-"
      },
      "outputs": [],
      "source": [
        "from transformers import LongformerTokenizer, LongformerModel\n",
        "import torch\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "def longformer_sample():\n",
        "  # Sample comments and their IDs\n",
        "  sample_comment = {\n",
        "      'id': 'iqmwd8f',\n",
        "      'body': \"I'm tired of this life. what should I do?\",\n",
        "      'author': 't2_57iup',\n",
        "      'reply_to': 'xsyg0d',\n",
        "      'conversation_id': 'xsyg0d'\n",
        "  }\n",
        "\n",
        "  sample_reply_to_comment = {\n",
        "      'id': 'xsyg0d',\n",
        "      'body': \"I'm so good at playing guitar\",\n",
        "      'author': 't2_57iup',\n",
        "      'reply_to': None,\n",
        "      'conversation_id': 'xsyg0d'\n",
        "  }\n",
        "\n",
        "  # Initialize the Longformer tokenizer and model\n",
        "  model_name = \"allenai/longformer-base-4096\"\n",
        "  tokenizer = LongformerTokenizer.from_pretrained(model_name)\n",
        "  model = LongformerModel.from_pretrained(model_name)\n",
        "\n",
        "  # Tokenize and encode the sample comment and its reply\n",
        "  sample_comment_encoded = tokenizer.encode(sample_comment['body'], add_special_tokens=True, max_length=4096, truncation=True, return_tensors=\"pt\")\n",
        "  sample_reply_encoded = tokenizer.encode(sample_reply_to_comment['body'], add_special_tokens=True, max_length=4096, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "  # Forward pass through the Longformer model for the sample comment and reply\n",
        "  with torch.no_grad():\n",
        "      comment_output = model(sample_comment_encoded)\n",
        "      reply_output = model(sample_reply_encoded)\n",
        "\n",
        "  # Extract embeddings for the [CLS] token (the first token in the sequence) for the sample comment and reply\n",
        "  comment_embedding = comment_output.last_hidden_state[:, 0, :].numpy()[0]\n",
        "  reply_embedding = reply_output.last_hidden_state[:, 0, :].numpy()[0]\n",
        "\n",
        "  print(comment_embedding)\n",
        "  # Calculate the cosine similarity between the sample comment and its reply\n",
        "  similarity = cosine_similarity([comment_embedding], [reply_embedding])\n",
        "\n",
        "  # Print the similarity and both texts\n",
        "  print(\"Cosine Similarity:\", similarity)\n",
        "  print(\"j similarity:\", jaccard_similarity(sample_comment['body'], sample_reply_to_comment['body']))\n",
        "  print(\"Comment:\", sample_comment['body'])\n",
        "  print(\"Reply:\", sample_reply_to_comment['body'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Reading from a file using Pickle\n",
        "with open(f\"{base_path}conversations.pkl\", 'rb') as file:\n",
        "    conversations = pickle.load(file)\n",
        "\n",
        "# Print the loaded variables\n",
        "print(\"Len conversations:\", len(conversations))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JiaUyjd2fecr",
        "outputId": "0b6f0255-e196-413b-ad0e-6bceff3a71cd"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Len conversations: 33493\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for id in conversations:\n",
        "  id_index_map = dict()\n",
        "  comments = conversations[id]['comments']\n",
        "  selected_comments = []\n",
        "  for comment in comments:\n",
        "    if not(comment['body'] == '[deleted]' or comment['body'].endswith('[removed]')):\n",
        "        id_index_map[comment['id']] = len(selected_comments)\n",
        "        selected_comments.append(comment['body'])\n",
        "  # Create a TF-IDF vectorizer\n",
        "  tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "  # Fit and transform the comments to obtain TF-IDF vectors\n",
        "  #print(conversations[id]['full_text'])\n",
        "  tfidf_matrix = tfidf_vectorizer.fit_transform(selected_comments)\n",
        "\n",
        "  # Calculate the cosine similarity between comments\n",
        "  cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
        "\n",
        "  for pair in conversations[id]['pairs']:\n",
        "    pair['relatedness_score'] = cosine_sim[id_index_map[pair['comment_id']]][id_index_map[pair['reply_id']]]\n",
        "  # Print the cosine similarity matrix\n",
        "  #print(\"Cosine Similarity Matrix:\")\n",
        "  #print(cosine_sim)"
      ],
      "metadata": {
        "id": "CdtDpnBQik8H"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Writing to a file using Pickle\n",
        "with open(f\"{base_path}conversations_with_relatedness.pkl\", 'wb') as file:\n",
        "    pickle.dump(conversations, file)\n",
        "\n",
        "print(\"Variables written to the file.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQHjTGLq9OSD",
        "outputId": "ddf76d08-0731-4b04-d532-287b372d260f"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Variables written to the file.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dictionary\n",
        "my_dict = {'a': 1, 'b': 2, 'c': 3}\n",
        "\n",
        "# Get a list of values\n",
        "values_list = list(my_dict.values())\n",
        "\n",
        "# Print the list of values\n",
        "print(values_list)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NYj27_Rz5eJ",
        "outputId": "329d431b-a551-40db-e9d2-3676c7f65322"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(flat_comments)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZvusoU-IjdR8",
        "outputId": "e56ff969-99da-4b8e-86a7-f79f3de0f302"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "542128"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "flat_comments[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6aBmcNkjmMb",
        "outputId": "64febfee-857c-472b-81dc-437a60c63c28"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'comment': 'MLB Sat Info\\n\\nPlay\\xa0OnAll underdogs with a money line of +100 or higher\\xa0(SEATTLE)team with a terrible SLG (&lt;=.400) against a very good starting pitcher (WHIP&lt;=1.300) -AL, with a very good bullpen whose WHIP is 1.250 or better on the season217-224\\xa0over the last 5 seasons.49.2%\\xa0(89.1\\xa0units)',\n",
              " 'reply': 'Play\\xa0OnAll underdogs with a money line of +100 or higher\\xa0(TAMPA BAY)team with a terrible SLG (<=.400) against a very good starting pitcher (WHIP<=1.300) -AL, with a very good bullpen whose WHIP is 1.250 or better on the season217-224\\xa0over the last 5 seasons.49.2%\\xa0(89.1\\xa0units)',\n",
              " 'comment_id': 'xyt38s',\n",
              " 'reply_id': 'iromk5a',\n",
              " 'global_id': 'xyt38s_iromk5a'}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iorSszcCvxwL",
        "outputId": "227d072d-026b-4942-ac7a-0dd147c9c733"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Similarity Matrix:\n",
            "[[1.         0.04307671 0.13535702]\n",
            " [0.04307671 1.         0.06419055]\n",
            " [0.13535702 0.06419055 1.        ]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import linear_kernel\n",
        "\n",
        "# Sample comments\n",
        "comments = [\n",
        "    \"I've been out looking for Ghoulia today, good to know why I couldn't find her.\",\n",
        "    \"Ghoulia is an elusive character, isn't she? Maybe she's hiding from the monsters.\",\n",
        "    \"I went for a walk in the park today.\",\n",
        "]\n",
        "\n",
        "# Create a TF-IDF vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform the comments to obtain TF-IDF vectors\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(comments)\n",
        "\n",
        "# Calculate the cosine similarity between comments\n",
        "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
        "\n",
        "# Print the cosine similarity matrix\n",
        "print(\"Cosine Similarity Matrix:\")\n",
        "print(cosine_sim)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOgFNEsgbuG6MHlWv5xOM8i",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}