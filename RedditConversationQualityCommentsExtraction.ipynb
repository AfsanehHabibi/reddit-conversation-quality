{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AfsanehHabibi/reddit-conversation-quality/blob/main/RedditConversationQualityCommentsExtraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiTOhyXkvh5Z"
      },
      "source": [
        "#Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92-VNbNwVPKI"
      },
      "outputs": [],
      "source": [
        "!pip install praw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7lMBMnTqWlhq"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iniJZxXlWh2y"
      },
      "outputs": [],
      "source": [
        "base_path = \"/content/drive/MyDrive/University/RedditData/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "osF-IcjYbPkC"
      },
      "outputs": [],
      "source": [
        "import praw\n",
        "import json\n",
        "from prawcore.exceptions import Forbidden\n",
        "import time\n",
        "import os\n",
        "import unittest\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "import signal\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "biBLTUzfVPoJ"
      },
      "outputs": [],
      "source": [
        "# Create a Reddit instance\n",
        "reddit = praw.Reddit(client_id='4Fh9K7U29aRt6GUpOkoqfw', client_secret='tyuF_v56l2bFmSqUOXFldWPtYsFViA', user_agent='Simurgh__')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeE6e-9ywJda"
      },
      "source": [
        "#Comment extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5sHTPS5JLJZ"
      },
      "outputs": [],
      "source": [
        "def write_data_to_file(element, entry, id):\n",
        "  with open(f\"{base_path}{entry}s_{id}.json\", \"a\") as f:\n",
        "      json.dump(element, f)\n",
        "      f.write(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1Mqx8P6Z335"
      },
      "outputs": [],
      "source": [
        "# Function to write the progress to a file\n",
        "def write_progress_to_file(last_processed_index, filename):\n",
        "    with open(filename, 'w') as f:\n",
        "        f.write(str(last_processed_index))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UCEF4wP0VIyd"
      },
      "outputs": [],
      "source": [
        "import praw\n",
        "import json\n",
        "import time\n",
        "\n",
        "def process_submission_comments(id):\n",
        "    # Load or create the progress file\n",
        "    progress_file = f'{base_path}comments_progress_{id}.txt'\n",
        "    if os.path.isfile(progress_file):\n",
        "        with open(progress_file, 'r') as f:\n",
        "            last_processed_index = int(f.read())\n",
        "            print(\"last_processed_index \", last_processed_index)\n",
        "    else:\n",
        "        last_processed_index = 0\n",
        "\n",
        "    # Open the file containing the submission IDs and number of comments\n",
        "    with open(f'{base_path}submissions_{id}.json', 'r') as f:\n",
        "        submission_data = [json.loads(line) for line in f]\n",
        "\n",
        "    # Iterate over the submission IDs and retrieve the comments for each submission\n",
        "    for index, data in enumerate(submission_data[last_processed_index:], start=last_processed_index ):\n",
        "        if last_processed_index != 0:\n",
        "            index += 1\n",
        "        #try:\n",
        "        #   submission_data[index]\n",
        "\n",
        "        #except IndexError:\n",
        "        #   print(\"extraction is over for id.\", id)\n",
        "        #   break\n",
        "        print(\"current submission index \", index)\n",
        "        # Get the submission ID and number of comments\n",
        "        submission_id = data['id']\n",
        "        problematic_ids = [\"xxjagq\", \"xyswda\", \"xysvkb\", \"xxjb63\", \"xsodkr\", \"xsobpb\", \"xuau8m\", \"xuf428\", \"xv1128\", \"xv100m\", \"xv0yjd\", \"xv0yiy\", \"xv0wu8\", \"xw5ezo\", \"xw5eo0\"]#xuf6e4 xv1128\n",
        "        if submission_id in problematic_ids:\n",
        "          print(\"skipping id \",submission_id, \" inaccessible for unknown reason ...\")\n",
        "          write_progress_to_file(index, progress_file)\n",
        "          continue\n",
        "        num_comments = data['num_comments']\n",
        "\n",
        "        if num_comments == 0:\n",
        "            print(f\"Submission ID {submission_id} has zero comments, skipping...\")\n",
        "            write_progress_to_file(index, progress_file)\n",
        "            continue\n",
        "        if num_comments > 1000:\n",
        "            print(f\"Submission ID {submission_id},  {num_comments} comments, skipping for now...\")\n",
        "            write_progress_to_file(index, progress_file)\n",
        "            continue\n",
        "\n",
        "        # Get the submission object\n",
        "        # print(\"id \", submission_id, \" num of comments \", num_comments)\n",
        "        try:\n",
        "            # Get the submission object\n",
        "            submission = reddit.submission(id=submission_id)\n",
        "            print(\"submission \", submission.id)\n",
        "            # Check if the submission is public and accessible\n",
        "            if submission.author is not None:\n",
        "                print(f\"Submission ID {submission_id} is public and accessible. with {num_comments} comments.\")\n",
        "            else:\n",
        "                print(f\"Submission ID {submission_id} is either private or inaccessible, skipping...\")\n",
        "                write_progress_to_file(index, progress_file)\n",
        "                continue\n",
        "\n",
        "        except Forbidden:\n",
        "            print(\"Access to the submission is forbidden, skipping...\")\n",
        "            write_progress_to_file(index, progress_file)\n",
        "            continue\n",
        "\n",
        "        while True:\n",
        "            try:\n",
        "                # Get all of the comments\n",
        "                submission.comments.replace_more(limit=None)\n",
        "                break\n",
        "            except Forbidden:\n",
        "                # If we get a 403 Forbidden error, wait for 10 seconds and try again\n",
        "                print(\"Got a 403 error. Waiting for 30 seconds and trying again...\")\n",
        "                time.sleep(30)\n",
        "        # Collect the comment data with specified keys\n",
        "        comments = []\n",
        "        comment_keys = ['all_awardings', 'approved_at_utc', 'approved_by', 'archived', 'associated_award', 'author', 'author_flair_background_color', 'author_flair_css_class', 'author_flair_richtext', 'author_flair_template_id', 'author_flair_text', 'author_flair_text_color', 'author_flair_type', 'author_fullname', 'author_is_blocked', 'author_patreon_flair', 'author_premium', 'award', 'awarders', 'banned_at_utc', 'banned_by', 'block', 'body', 'body_html', 'can_gild', 'can_mod_post', 'clear_vote', 'collapse', 'collapsed', 'collapsed_because_crowd_control', 'collapsed_reason', 'collapsed_reason_code', 'comment_type', 'controversiality', 'created', 'created_utc', 'delete', 'depth', 'disable_inbox_replies', 'distinguished', 'downs', 'downvote', 'edit', 'edited', 'enable_inbox_replies', 'fullname', 'gild', 'gilded', 'gildings', 'id', 'id_from_url', 'is_root', 'is_submitter', 'likes', 'link_id', 'locked', 'mark_read', 'mark_unread', 'mod', 'mod_note', 'mod_reason_by', 'mod_reason_title', 'mod_reports', 'name', 'no_follow', 'num_reports', 'parent', 'parent_id', 'parse', 'permalink', 'refresh', 'removal_reason', 'replies', 'reply', 'report', 'report_reasons', 'save', 'saved', 'score', 'score_hidden', 'send_replies', 'stickied', 'submission', 'top_awarded_type', 'total_awards_received', 'treatment_tags', 'uncollapse', 'unrepliable_reason', 'unsave', 'ups', 'upvote', 'user_reports']\n",
        "\n",
        "        for comment in submission.comments.list():\n",
        "            comment_data = {}\n",
        "            for key in comment_keys:\n",
        "                if hasattr(comment, key):\n",
        "                    obj = getattr(comment, key)\n",
        "                    if isinstance(obj, dict) or isinstance(obj, list) or isinstance(obj, int) or isinstance(obj, float) or isinstance(obj, str) or isinstance(obj, bool):\n",
        "                        comment_data[key] = getattr(comment, key)\n",
        "            comments.append(comment_data)\n",
        "\n",
        "        # Create the submission data\n",
        "        submission_data = {\n",
        "            'submission_id': submission_id,\n",
        "            'num_comments': num_comments,\n",
        "            'comments': comments\n",
        "        }\n",
        "\n",
        "        # Save the comment data to a JSON file\n",
        "        write_data_to_file(submission_data, \"comment\", id)\n",
        "        # Update the progress file with the current index\n",
        "        write_progress_to_file(index, progress_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbCJI4nTwS9-"
      },
      "source": [
        "#Executor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9aKZyGCdh9av"
      },
      "outputs": [],
      "source": [
        "# Define the function that executes another function with a timeout\n",
        "def execute_with_timeout(func, timeout_duration, *args, **kwargs):\n",
        "    # Define the handler for the interrupt signal\n",
        "    def handler(signum, frame):\n",
        "        raise TimeoutError(\"Function execution timed out\")\n",
        "\n",
        "    # Set the signal handler\n",
        "    signal.signal(signal.SIGALRM, handler)\n",
        "\n",
        "    try:\n",
        "        # Start the timer\n",
        "        signal.alarm(timeout_duration)\n",
        "\n",
        "        # Execute the function with the provided arguments\n",
        "        func(*args, **kwargs)\n",
        "\n",
        "    except TimeoutError:\n",
        "        # Handle the timeout error\n",
        "        print(\"Function execution timed out\")\n",
        "\n",
        "    finally:\n",
        "        # Disable the alarm\n",
        "        signal.alarm(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqBxUKwHd9sb"
      },
      "outputs": [],
      "source": [
        "def copy_submission_id(submission_progress_filename, comment_progress_file_name):\n",
        "    with open(submission_progress_filename, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    with open(comment_progress_file_name, 'w') as new_file:\n",
        "        for line in lines:\n",
        "            values = line.strip().split(',')\n",
        "            id = values[0].strip()\n",
        "            new_line = f\"{id},0\\n\"\n",
        "            new_file.write(new_line)\n",
        "            new_file.flush()\n",
        "\n",
        "    print(f\"New file '{comment_progress_file_name}' created with the copied IDs.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "msLbaMyFf5w0"
      },
      "outputs": [],
      "source": [
        "def update_comment_progress_file(file_path, id, status):\n",
        "    print(\"file path\", file_path, \" id \", id, \" status \", status)\n",
        "    # Read the contents of the file\n",
        "    with open(file_path, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    # Find the line with the starting timestamp\n",
        "    for i, line in enumerate(lines):\n",
        "        line_id, old_status = line.strip().split(',')\n",
        "        if line_id == id:\n",
        "            # Update the status to the new value\n",
        "            lines[i] = f\"{line_id},{status}\\n\"\n",
        "            break\n",
        "\n",
        "    # Write the updated contents back to the file\n",
        "    with open(file_path, 'w') as f:\n",
        "        f.writelines(lines)\n",
        "        f.flush()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eAfxyz4pXztB"
      },
      "outputs": [],
      "source": [
        "def extract(submission_file_path, processor_func):\n",
        "    file_path = f'{submission_file_path}_comments_progress.txt'\n",
        "    if not os.path.exists(file_path):\n",
        "        copy_submission_id(submission_file_path, file_path)\n",
        "\n",
        "    iterated_ids = []\n",
        "    while True:\n",
        "        all_done = True\n",
        "        with open(file_path, \"r\") as f:\n",
        "            for line in f:\n",
        "                id, done_str = line.strip().split(\",\")\n",
        "                done = int(done_str) == 1\n",
        "                if not done:\n",
        "                    all_done = False\n",
        "                    break\n",
        "        if all_done:\n",
        "          break\n",
        "        processor_func(id)\n",
        "        iterated_ids.append(id)\n",
        "        update_comment_progress_file(file_path, id, 1)\n",
        "    return iterated_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jap_jdP2m4dZ"
      },
      "outputs": [],
      "source": [
        "import unittest\n",
        "import os\n",
        "\n",
        "# Define a test class\n",
        "class ExtractTestCase(unittest.TestCase):\n",
        "    # Set up any necessary test data before each test case\n",
        "    def setUp(self):\n",
        "        # Create a test submission file with 20 lines\n",
        "        self.submission_file_path = 'test_submission_file.txt'\n",
        "        self.progress_file_path = f'{self.submission_file_path}_comments_progress.txt'\n",
        "        with open(self.submission_file_path, 'w') as file:\n",
        "            for i in range(20):\n",
        "                file.write(f\"{i+1},{i+1},0\\n\")\n",
        "        if os.path.exists(self.progress_file_path):\n",
        "            os.remove(self.progress_file_path)\n",
        "\n",
        "    # Clean up any resources after each test case\n",
        "    def tearDown(self):\n",
        "        # Remove the test submission file\n",
        "        if os.path.exists(self.submission_file_path):\n",
        "            os.remove(self.submission_file_path)\n",
        "        if os.path.exists(self.progress_file_path):\n",
        "            os.remove(self.progress_file_path)\n",
        "\n",
        "    # Test scenario 1: Progress file doesn't exist at the start\n",
        "    def test_progress_file_creation(self):\n",
        "        print(\"test_progress_file_creation\")\n",
        "        # Ensure the progress file doesn't exist initially\n",
        "        self.assertFalse(os.path.exists(self.progress_file_path))\n",
        "\n",
        "        # Call the extract function\n",
        "        result = extract(self.submission_file_path, lambda x: None)\n",
        "\n",
        "        # Ensure the progress file is created\n",
        "        self.assertTrue(os.path.exists(self.progress_file_path))\n",
        "\n",
        "        # Ensure the extract function iterated through all IDs (lines)\n",
        "        self.assertEqual(len(result), 20)\n",
        "\n",
        "    # Test scenario 2: Set progress status to 1 for some lines\n",
        "    def test_set_progress_status(self):\n",
        "        print(\"test_set_progress_status\")\n",
        "        # Create the progress file with some lines set to 1\n",
        "        copy_submission_id(self.submission_file_path, self.progress_file_path)\n",
        "        update_comment_progress_file(self.progress_file_path, \"5\", 1)\n",
        "        update_comment_progress_file(self.progress_file_path, \"10\", 1)\n",
        "\n",
        "        # Call the extract function\n",
        "        result = extract(self.submission_file_path, lambda x: None)\n",
        "\n",
        "        # Ensure the extract function iterated from line 6 to line 20\n",
        "        self.assertEqual(len(result), 18)\n",
        "\n",
        "    # Test scenario 3: Interrupt function execution and check progress file changes\n",
        "    def test_interrupt_function_execution(self):\n",
        "        print(\"test_interrupt_function_execution\")\n",
        "        # Define a mock function to be passed as an argument\n",
        "        def mock_func(data_id):\n",
        "            print(f\"Interupted, Processing data ID: {data_id}\")\n",
        "            # Simulate a long execution\n",
        "            time.sleep(1)\n",
        "\n",
        "        # Call the function being tested\n",
        "        execute_with_timeout(extract, 5, self.submission_file_path, mock_func)\n",
        "\n",
        "        # Check the progress file for changes\n",
        "        with open(self.progress_file_path, 'r') as file:\n",
        "            lines = file.readlines()\n",
        "\n",
        "        # Ensure the progress status of all lines is 1\n",
        "        processed = 0\n",
        "        for line in lines:\n",
        "           id, status = line.strip().split(',')\n",
        "           if status == \"1\":\n",
        "              processed += 1\n",
        "        # Call the extract function with the mock processor function\n",
        "        output = extract(self.submission_file_path, mock_func)\n",
        "        self.assertEqual(processed + len(output), 20)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyOkpyHSyiTh"
      },
      "source": [
        "#Extraction call"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2b_fAn4yyhAy"
      },
      "outputs": [],
      "source": [
        "from prawcore.exceptions import Forbidden, TooManyRequests\n",
        "while True:\n",
        "  try:\n",
        "    extract(f\"{base_path}timestamps_seed2_num20_period10_date2022-10-1.txt\", process_submission_comments)\n",
        "  except TooManyRequests:\n",
        "    # If we get a 429 TooManyRequests error, wait for 60 seconds and try again\n",
        "    print(\"Got a 429 error (Too Many Requests). Waiting for 60 seconds and trying again...\")\n",
        "    time.sleep(60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tERUEA7lYugC"
      },
      "source": [
        "#Test execution call"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7RM-xS7wpabU"
      },
      "outputs": [],
      "source": [
        "test_case = ExtractTestCase()\n",
        "unittest.main(argv=[''], exit=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1m1VA0Jwh9l"
      },
      "source": [
        "#Comments key processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MqqwYuZIfODL"
      },
      "outputs": [],
      "source": [
        "import praw\n",
        "import openpyxl\n",
        "import os.path\n",
        "\n",
        "# Check if the comment keys file exists\n",
        "if os.path.exists(f'{base_path}comment_keys.xlsx'):\n",
        "    print(\"Comment keys file already exists. Skipping rewriting.\")\n",
        "else:\n",
        "    # Create a Reddit instance\n",
        "    reddit = praw.Reddit(client_id='4Fh9K7U29aRt6GUpOkoqfw', client_secret='tyuF_v56l2bFmSqUOXFldWPtYsFViA', user_agent='Simurgh__')\n",
        "\n",
        "    # Get a submission\n",
        "    submission_id = 'xz45qo'\n",
        "    submission = reddit.submission(id=submission_id)\n",
        "\n",
        "    # Get a comment\n",
        "    comment = submission.comments.list()[0]  # Get the first comment\n",
        "\n",
        "\n",
        "    # Load submission fields from submission_keys.xlsx\n",
        "    submission_workbook = openpyxl.load_workbook(f'{base_path}submission_keys.xlsx')\n",
        "    submission_sheet = submission_workbook.active\n",
        "\n",
        "    # Create a new Excel workbook\n",
        "    comment_workbook = openpyxl.Workbook()\n",
        "    comment_sheet = comment_workbook.active\n",
        "\n",
        "    # Set the first row as header\n",
        "    comment_sheet.cell(row=1, column=1).value = \"field\"\n",
        "\n",
        "    # Write the comment keys to the first column\n",
        "    fields = dir(comment)\n",
        "    row_index = 2\n",
        "    for field in fields:\n",
        "        if not field.startswith('_'):\n",
        "            comment_sheet.cell(row=row_index, column=1).value = field\n",
        "            comment_sheet.cell(row=row_index, column=2).value = field in [cell.value for cell in submission_sheet['A']]\n",
        "            row_index += 1\n",
        "\n",
        "    # Save the workbook as an Excel file\n",
        "    comment_workbook.save(f'{base_path}comment_keys.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_bnRdGTF8B6o"
      },
      "outputs": [],
      "source": [
        "import openpyxl\n",
        "\n",
        "# Load the comment keys Excel file\n",
        "workbook = openpyxl.load_workbook(f'{base_path}comment_keys.xlsx')\n",
        "sheet = workbook.active\n",
        "\n",
        "# Initialize an empty array to store the fields\n",
        "fields = []\n",
        "\n",
        "# Iterate over the rows starting from the second row\n",
        "for row in sheet.iter_rows(min_row=2, values_only=True):\n",
        "    comment_field = row[0]\n",
        "    exists_in_submission = row[2]\n",
        "\n",
        "    # Check if the value in the third column is FALSE\n",
        "    if exists_in_submission is False:\n",
        "        fields.append(comment_field)\n",
        "\n",
        "# Print the fields in an array-like string\n",
        "print(\"Fields with value FALSE in the third column:\")\n",
        "print(str(fields))\n",
        "print(len(fields))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPT3PxdoIa1DwaP6FNEZ+O/",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}