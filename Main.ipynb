{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AfsanehHabibi/reddit-conversation-quality/blob/main/Main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_nwsrw4kRxSO",
        "outputId": "1ee67902-7c54-4a5d-fcf4-ae14d498d57c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting anytree\n",
            "  Downloading anytree-2.12.1-py3-none-any.whl (44 kB)\n",
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/44.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.9/44.9 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from anytree) (1.16.0)\n",
            "Installing collected packages: anytree\n",
            "Successfully installed anytree-2.12.1\n"
          ]
        }
      ],
      "source": [
        "!pip install anytree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpq42XsomqOz",
        "outputId": "ca93e09e-224e-4195-c04b-1bdd8b0844d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect) (1.16.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993225 sha256=154d060cef5f68001c2ec3c58bf8a463bc0b626895485657b697e35f937f9556\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n"
          ]
        }
      ],
      "source": [
        "!pip install langdetect"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install  kili"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5uUiR7-SVXY",
        "outputId": "09e2a89c-7686-4c5a-ad3b-ec53e923008f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kili\n",
            "  Downloading kili-2.149.0-py3-none-any.whl (288 kB)\n",
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/288.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m122.9/288.4 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m288.4/288.4 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas<3.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from kili) (1.5.3)\n",
            "Requirement already satisfied: click<9.0.0,>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from kili) (8.1.7)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from kili) (2.31.0)\n",
            "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from kili) (0.9.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from kili) (8.2.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from kili) (4.66.1)\n",
            "Collecting typeguard<5,>=4 (from kili)\n",
            "  Downloading typeguard-4.1.5-py3-none-any.whl (34 kB)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from kili) (4.5.0)\n",
            "Requirement already satisfied: pyparsing<4.0.0,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from kili) (3.1.1)\n",
            "Requirement already satisfied: websocket-client<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from kili) (1.7.0)\n",
            "Requirement already satisfied: pyyaml<7.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from kili) (6.0.1)\n",
            "Requirement already satisfied: Pillow<11.0.0,>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from kili) (9.4.0)\n",
            "Collecting cuid<0.5,>=0.4 (from kili)\n",
            "  Downloading cuid-0.4.tar.gz (5.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from kili) (2.0.7)\n",
            "Collecting ffmpeg-python<0.3.0,>=0.2.0 (from kili)\n",
            "  Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
            "Collecting gql[requests,websockets]<4.0.0,>=3.5.0b5 (from kili)\n",
            "  Downloading gql-3.6.0b0-py2.py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m74.0/74.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock<4.0.0,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from kili) (3.13.1)\n",
            "Collecting pyrate-limiter<3,>=2 (from kili)\n",
            "  Downloading pyrate_limiter-2.10.0-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from ffmpeg-python<0.3.0,>=0.2.0->kili) (0.18.3)\n",
            "Collecting graphql-core<3.4,>=3.3.0a3 (from gql[requests,websockets]<4.0.0,>=3.5.0b5->kili)\n",
            "  Downloading graphql_core-3.3.0a3-py3-none-any.whl (209 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m209.9/209.9 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: yarl<2.0,>=1.6 in /usr/local/lib/python3.10/dist-packages (from gql[requests,websockets]<4.0.0,>=3.5.0b5->kili) (1.9.4)\n",
            "Collecting backoff<3.0,>=1.11.1 (from gql[requests,websockets]<4.0.0,>=3.5.0b5->kili)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gql[requests,websockets]<4.0.0,>=3.5.0b5->kili) (3.7.1)\n",
            "Collecting requests-toolbelt<2,>=1.0.0 (from gql[requests,websockets]<4.0.0,>=3.5.0b5->kili)\n",
            "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets<12,>=10 (from gql[requests,websockets]<4.0.0,>=3.5.0b5->kili)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0.0,>=1.0.0->kili) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0.0,>=1.0.0->kili) (2023.3.post1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0.0,>=1.0.0->kili) (1.23.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->kili) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->kili) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->kili) (2023.11.17)\n",
            "Collecting typing-extensions<5.0.0,>=4.1.0 (from kili)\n",
            "  Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.0->gql[requests,websockets]<4.0.0,>=3.5.0b5->kili) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.0->gql[requests,websockets]<4.0.0,>=3.5.0b5->kili) (1.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas<3.0.0,>=1.0.0->kili) (1.16.0)\n",
            "Requirement already satisfied: multidict>=4.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.6->gql[requests,websockets]<4.0.0,>=3.5.0b5->kili) (6.0.4)\n",
            "Building wheels for collected packages: cuid\n",
            "  Building wheel for cuid (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cuid: filename=cuid-0.4-py2.py3-none-any.whl size=4714 sha256=f2b3c543a2cc5ebdb78caf73b7dc3fd31537448cea5c6ff4d3814e675ca569a6\n",
            "  Stored in directory: /root/.cache/pip/wheels/52/0a/dc/a0e28c435d5a74d9eef3d7c3cd147b96cb21e71e5ec7dcfdbe\n",
            "Successfully built cuid\n",
            "Installing collected packages: cuid, websockets, typing-extensions, pyrate-limiter, graphql-core, ffmpeg-python, backoff, typeguard, requests-toolbelt, gql, kili\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed backoff-2.2.1 cuid-0.4 ffmpeg-python-0.2.0 gql-3.6.0b0 graphql-core-3.3.0a3 kili-2.149.0 pyrate-limiter-2.10.0 requests-toolbelt-1.0.0 typeguard-4.1.5 typing-extensions-4.9.0 websockets-11.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2nKEu6z62yR",
        "outputId": "e272c6e4-174b-4a8a-a7a8-d919837f781f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_70YRYLYNso"
      },
      "source": [
        "#Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g3cAemLLTz6h"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import json\n",
        "from anytree import Node, RenderTree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WF_hZIxO64-f"
      },
      "outputs": [],
      "source": [
        "base_path = \"/content/drive/MyDrive/University/RedditData/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZHFK04u4W1_r"
      },
      "outputs": [],
      "source": [
        "def extract_files_list(file_list_path):\n",
        "    files_name = []\n",
        "    with open(file_list_path, 'r') as file_list:\n",
        "        for line in file_list:\n",
        "            line = line.strip()\n",
        "            if line:\n",
        "                file_path, _ = line.split(',')\n",
        "                files_name.append(file_path)\n",
        "    return files_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wxmCIBekXfCX"
      },
      "outputs": [],
      "source": [
        "data_files_name = extract_files_list(f\"{base_path}timestamps_seed2_num20_period10_date2022-10-1.txt_comments_progress.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZTRmfusY7K4"
      },
      "outputs": [],
      "source": [
        "def make_submission_file_path(file_name):\n",
        "    return f\"{base_path}submissions_{file_name}.json\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ybXnmbWIgl-D"
      },
      "outputs": [],
      "source": [
        "def make_comment_file_path(file_name):\n",
        "    return f\"{base_path}comments_{file_name}.json\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36MQkOALazZV"
      },
      "outputs": [],
      "source": [
        "subreddits = set()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-EDIPZXzYJjf"
      },
      "outputs": [],
      "source": [
        "def extract_submissions_text_with_filter(data_files_name):\n",
        "    submissions_dic = {}\n",
        "    over_18_count = 0\n",
        "    for file_name in data_files_name:\n",
        "        file_path = make_submission_file_path(file_name)\n",
        "        with open(file_path, 'r') as file:\n",
        "            lines_sub = file.readlines()\n",
        "        for l in lines_sub:\n",
        "            obj = json.loads(l)\n",
        "            if obj['over_18'] == False:\n",
        "                submissions_dic[obj['id']]  = obj['title'] + obj['selftext']\n",
        "                subreddits.add(obj['subreddit'])\n",
        "            else:\n",
        "                over_18_count += 1\n",
        "    print(\"over 18 count \", over_18_count)\n",
        "    return submissions_dic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PyrYu-8qbO9i",
        "outputId": "58c41382-3e68-4997-8859-e445c00f6ac8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "over 18 count  66876\n"
          ]
        }
      ],
      "source": [
        "subsmissions = extract_submissions_text_with_filter(data_files_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzw-iWqGcN8J",
        "outputId": "eaf1a657-e18d-4be6-bec1-67ff0d870b8f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "96333"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "len(subsmissions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-gatXoaOEzjc"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def count_comments_in_file(file_path):\n",
        "    count = 0\n",
        "    with open(file_path, 'r') as json_file:\n",
        "        lines = json_file.readlines()\n",
        "        for line in lines:\n",
        "            data = json.loads(line)\n",
        "            count += len(data.get('comments', []))\n",
        "    return count\n",
        "\n",
        "def total_comments_in_files(file_list_path):\n",
        "    total_comments = 0\n",
        "    with open(file_list_path, 'r') as file_list:\n",
        "        for line in file_list:\n",
        "            print(line)\n",
        "            line = line.strip()\n",
        "            if line:\n",
        "                file_path, _ = line.split(',')\n",
        "                total_comments += count_comments_in_file(f\"{base_path}comments_{file_path}.json\")\n",
        "    return total_comments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5v3l38OFerIb"
      },
      "outputs": [],
      "source": [
        "def load_comments_with_filter(files_name, submmissions):\n",
        "    # Create a dictionary to store the comment nodes by their IDs\n",
        "    conversations_dic = {}\n",
        "    for file_name in data_files_name:\n",
        "        comment_file = make_comment_file_path(file_name)\n",
        "        with open(comment_file, 'r') as file:\n",
        "            lines = file.readlines()\n",
        "\n",
        "        for line in lines:\n",
        "            comment_nodes = {}\n",
        "            json_obj = json.loads(line)\n",
        "            submission_id = json_obj['submission_id']\n",
        "            comments = json_obj['comments']\n",
        "            if len(comments) == 0:\n",
        "              continue\n",
        "            if submission_id not in subsmissions:\n",
        "                #submission has removed in preprocessing\n",
        "                continue\n",
        "            flat_comments_list = []\n",
        "            parent_child_pairs = []\n",
        "            # Create root node for the submission\n",
        "            submission_text = subsmissions[submission_id]\n",
        "            if submission_text.endswith('[removed]'):\n",
        "              continue\n",
        "            root = Node(submission_id, body = submission_text)\n",
        "            comment_nodes[submission_id] = root\n",
        "            conversation_text = submission_text\n",
        "            flat_comments_list.append({'id': submission_id, 'body': submission_text, 'author':None, 'reply_to':None, 'conversation_id':submission_id})\n",
        "            # Create child nodes for the comments\n",
        "            for comment in comments:\n",
        "                # Check if comment is from a bot and ignore it if it is\n",
        "                if \"I am a bot, and this action was performed automatically\" in comment['body']:\n",
        "                    continue\n",
        "\n",
        "                comment_id = comment['id']\n",
        "                author = comment.get('author_fullname', 'unknown')\n",
        "                parent_id = comment['parent_id'].split('_', 1)[-1]\n",
        "                body = comment['body']\n",
        "                comment_node = Node(comment_id, body=body)\n",
        "\n",
        "                if parent_id in comment_nodes:\n",
        "                    flat_comments_list.append({'id': comment_id, 'body': body, 'author':author, 'reply_to':parent_id, 'conversation_id':submission_id})\n",
        "                    parent_node = comment_nodes[parent_id]\n",
        "                    parent_child_pairs.append({\n",
        "                        'comment': parent_node.body,\n",
        "                        'reply': body,\n",
        "                        'comment_id': parent_node.name,\n",
        "                        'reply_id': comment_id,\n",
        "                        'global_id': parent_node.name + \"_\" + comment_id\n",
        "                    })\n",
        "                    comment_node.parent = parent_node\n",
        "                    conversation_text += \"/n\"+ body\n",
        "                    comment_nodes[comment_id] = comment_node\n",
        "            # Print the tree structure\n",
        "            # print(RenderTree(root))\n",
        "            conversations_dic[submission_id] = {\n",
        "                'tree': root,\n",
        "                'full_text': conversation_text,\n",
        "                'pairs': parent_child_pairs,\n",
        "                'comments': flat_comments_list\n",
        "            }\n",
        "            #for parent in root.children:\n",
        "            #  for child in parent.children:\n",
        "            #       parent_child_pairs.append((parent.body, child.body))\n",
        "\n",
        "            # Print the parent-child pairs\n",
        "            #for pair in parent_child_pairs:\n",
        "            #   print(f\"Parent: {pair[0]}, Child: {pair[1]}\")\n",
        "\n",
        "            # Print the tree structure\n",
        "            #for pre, fill, node in RenderTree(root):\n",
        "            #   print(f\"{pre}{node.name}\")\n",
        "\n",
        "        #print(RenderTree(conversations_tree[2555]))\n",
        "    return conversations_dic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKtZ62JIixvx"
      },
      "outputs": [],
      "source": [
        "conversations_dic = load_comments_with_filter(data_files_name, subsmissions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffWeiRW-j-Ij",
        "outputId": "172992bd-249f-429a-e645-9a573e85fb51"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "37340"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "len(conversations_dic)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HmgKU0v5yaGw"
      },
      "outputs": [],
      "source": [
        "def filter_empty_conversations(conversations_dic):\n",
        "  non_empty_conversations = {}\n",
        "  for id in conversations_dic:\n",
        "    new_pairs = []\n",
        "    new_comments = []\n",
        "    pairs = conversations_dic[id]['pairs']\n",
        "    for pair in pairs:\n",
        "      #use endswith instead of eqality check cause if a submission is removed its title might be still there\n",
        "      if not(pair['comment'] == '[deleted]' or pair['reply'] == '[deleted]' or pair['comment'] == '[removed]' or pair['reply'] == '[removed]'):\n",
        "        new_pairs.append(pair)\n",
        "    # should i remove comments which are deleted or removed? problem is later on when measuring politness it need a whole graph and if i remove these it ends up with ids pointing to nothing\n",
        "    if len(new_pairs) > 0:\n",
        "      conversations_dic[id]['pairs'] = new_pairs\n",
        "      non_empty_conversations[id] = conversations_dic[id]\n",
        "  return non_empty_conversations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZLG3P45O53NQ"
      },
      "outputs": [],
      "source": [
        "non_empty_conversations = filter_empty_conversations(conversations_dic)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8KEfKfH-GQP",
        "outputId": "8a7ea98b-71a5-41bb-d0c8-2d369b15b31f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "34782"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "len(non_empty_conversations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4c4sbU4nK5_"
      },
      "outputs": [],
      "source": [
        "from langdetect import detect, LangDetectException\n",
        "\n",
        "def filter_non_english(conversations_dic):\n",
        "    english_conversations = {}\n",
        "    for id in conversations_dic:\n",
        "        try:\n",
        "            language = detect(conversations_dic[id]['full_text'])\n",
        "            if language == 'en':\n",
        "                english_conversations[id] = conversations_dic[id]\n",
        "        except LangDetectException as e:\n",
        "            # Handle the exception and print the problematic input\n",
        "            # print(f\"Language detection error for comment: {conversations_dic[id]['full_text']}\")\n",
        "            continue\n",
        "    return english_conversations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nD_P8W5qWsHb"
      },
      "outputs": [],
      "source": [
        "english_conversations = filter_non_english(non_empty_conversations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yk6xYJSqWz0y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a6b6a2b-7da1-422f-a040-a1cedb69eba1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32990"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "len(english_conversations)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Writing to a file using Pickle\n",
        "with open(f\"{base_path}conversations.pkl\", 'wb') as file:\n",
        "    pickle.dump(english_conversations, file)\n",
        "\n",
        "print(\"Variables written to the file.\")\n",
        "\n",
        "# Reading from a file using Pickle\n",
        "with open(f\"{base_path}conversations.pkl\", 'rb') as file:\n",
        "    loaded_data = pickle.load(file)\n",
        "\n",
        "# Print the loaded variables\n",
        "print(\"Len loaded data:\", len(loaded_data))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etFIZhgtV742",
        "outputId": "918b4028-48ff-47d1-c51f-0761564c5d38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Variables written to the file.\n",
            "Len loaded data: 32990\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RwV-WRFiQHxf",
        "outputId": "aeee6a18-1fd8-4638-8a31-bd7d10b430c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The text contains a question.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "def contains_question(text):\n",
        "    # Check if the text contains a question mark\n",
        "    if '?' in text:\n",
        "        return True\n",
        "\n",
        "    # Check if the text contains certain question words\n",
        "    question_words = ['what', 'how', 'when', 'where', 'who', 'why']\n",
        "    for word in question_words:\n",
        "        if re.search(r'\\b{}\\b'.format(word), text, re.IGNORECASE):\n",
        "            return True\n",
        "\n",
        "    # Check if the text contains the word \"please\"\n",
        "    if re.search(r'\\bplease\\b', text, re.IGNORECASE):\n",
        "        return True\n",
        "\n",
        "    # If none of the above conditions are met, return False\n",
        "    return False\n",
        "\n",
        "# Example usage\n",
        "text = \"What is the capital of France? I love Paris. How do you make a cake? The recipe is easy to follow. Can you please pass the salt?\"\n",
        "if contains_question(text):\n",
        "    print(\"The text contains a question.\")\n",
        "else:\n",
        "    print(\"The text does not contain a question.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGRc-uliMFrf",
        "outputId": "67d69185-350d-4460-fc27-4cb19fae932f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "comment: Meet up before the show?Hey, guys! Very much looking forward to these Red Rocks shows! I know it's been asked a lot on here already but would anyone be interested in meeting up for a drink on Saturday or Monday before the show? \n",
            "\n",
            "I work a ton and won't be able to make it to the meetup on Sunday...\n",
            "reply: Yes! Iâ€™ll PM you!!\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "random_pair = random.choice(list(english_conversations.items()))\n",
        "for pair in random_pair[1]['pairs']:\n",
        "  print(contains_question(pair['comment']))\n",
        "  print(\"comment:\", pair['comment'])\n",
        "  print(\"reply:\", pair['reply'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "omP3WZ5Rcflz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "000521a3-c0ca-46a8-b353-6798719a7633"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-33-0df81c083ef2>:9: DeprecationWarning: Sampling from a set deprecated\n",
            "since Python 3.9 and will be removed in a subsequent version.\n",
            "  random_sample = random.sample(english_conversations.items(), sample_size)\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "\n",
        "# Define the sample size (e.g., 3 for a sample of 3 elements)\n",
        "sample_size = 100\n",
        "random_pairs = []\n",
        "\n",
        "# Use random.sample to get a random sample of the specified size\n",
        "random_sample = random.sample(english_conversations.items(), sample_size)\n",
        "for sample_conversation in random_sample:\n",
        "    random_pair = random.choice(sample_conversation[1]['pairs'])\n",
        "    random_pairs.append(random_pair)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7RkZbfQdOuv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f48bd2d3-6876-4669-bbc9-bd17956ebeb1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "len(random_pairs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1CjlutGLdVOM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30a730e0-f251-4032-e34a-f54fd23fdffb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'comment': 'Does not make it okay, does it?',\n",
              "  'reply': 'â€œitâ€™s not okay.â€ is about the worst argument that anyone can come up with in relation to literally any problem.',\n",
              "  'comment_id': 'iqz6fvw',\n",
              "  'reply_id': 'iqz94rp',\n",
              "  'global_id': 'iqz6fvw_iqz94rp'},\n",
              " {'comment': 'TrueNAS + Plex: How do get Plex to see my files?Apologies if this is not the correct subreddit.\\n\\nI have Plex set up through my TrueNAS server. I have two pools - GLaDOS and tank. GLaDOS holds the media files (movies ,tv shows, music), and tank holds the jail for Plex.\\n\\nI have mount points set up to create a link between tank and GLaDOS so that way Plex sees the folders for the libraries. I also had to give full access to @everyone in order for this to happen.\\n\\nNow, Plex sees the media folders- and it sees the files inside of the folders, but when I perform a library scan - it does not populate the Plex files in the library.\\nHere is an [imgur album](https://imgur.com/a/NurYIOc) of what I am describing. \\n\\nHow do I get Plex to see these files in a way where they will show in the library?\\n\\nThank you for the help and assistance.',\n",
              "  'reply': 'The first thing coming to my mind is permissions and ownership issues. While Plex can see the files it might not be able to read them.\\n\\nThe other thing is your naming convention. Name your files correctly and you will prevent a lot of problems down the road.\\n\\nhttps://support.plex.tv/articles/naming-and-organizing-your-movie-media-files/',\n",
              "  'comment_id': 'xslf5y',\n",
              "  'reply_id': 'iqlcs15',\n",
              "  'global_id': 'xslf5y_iqlcs15'},\n",
              " {'comment': \"fafsaso if i know that i'm getting full tuition covered in state through national merit (benequisto for fl) then do i still have to list in state schools cause i don't have enough space\",\n",
              "  'reply': 'You can submit an edit to the FAFSA later that adds more schools - they provide instructions at the below link.\\n\\nhttps://studentaid.gov/help/more-ten-colleges',\n",
              "  'comment_id': 'xsyegf',\n",
              "  'reply_id': 'iqmvyao',\n",
              "  'global_id': 'xsyegf_iqmvyao'},\n",
              " {'comment': 'smileğŸ™‚',\n",
              "  'reply': 'God awful post',\n",
              "  'comment_id': 'xy5x13',\n",
              "  'reply_id': 'irgbc78',\n",
              "  'global_id': 'xy5x13_irgbc78'},\n",
              " {'comment': \"'Gladio': The clandestine military network operating on European soil for 40 years\",\n",
              "  'reply': \"Its an interesting question with regard to the overlap of UK's Jeremy Corbyn, earlier Labour Party leader and Gladio.  It seemed that the criticism of his every move, every breath, in the mainstream press and concerted efforts to bring him down were unprecedented and strange in their degree of obsession.\",\n",
              "  'comment_id': 'xuax42',\n",
              "  'reply_id': 'iqvlajw',\n",
              "  'global_id': 'xuax42_iqvlajw'}]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "random_pairs[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVwzZgyHdRg8",
        "outputId": "7aaeb000-3dae-4ff8-bffd-c9b049818fd2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Project ID:  clp5pxaet1wdn086r7y3x0ngo\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "from kili.client import Kili\n",
        "kili = Kili(api_key='6acd489f-9746-4ac4-9650-54e2d9c9faba')\n",
        "interface = {\n",
        "    \"jobs\": {\n",
        "        \"JOB_0\": {\n",
        "            \"mlTask\": \"CLASSIFICATION\",\n",
        "            \"required\": 1,\n",
        "            \"isChild\": False,\n",
        "            \"content\": {\n",
        "                \"categories\": {\"IsConversation\": {\"name\": \"has a common purpose or set of purposes, or at least a mutually accepted direction\"}, \"NotConversation\": {\"name\": \"has no prupose or a direction\"}},\n",
        "                \"input\": \"radio\",\n",
        "            },\n",
        "        },\n",
        "        \"JOB_1\": {\n",
        "            \"mlTask\": \"CLASSIFICATION\",\n",
        "            \"required\": 1,\n",
        "            \"isChild\": False,\n",
        "            \"content\": {\n",
        "                \"categories\": {\"Related\": {\"name\": \"is relevent\"}, \"UnRelated\": {\"name\": \"is not relevent\"}},\n",
        "                \"input\": \"radio\",\n",
        "            },\n",
        "        },\n",
        "        \"JOB_2\": {\n",
        "            \"mlTask\": \"CLASSIFICATION\",\n",
        "            \"required\": 1,\n",
        "            \"isChild\": False,\n",
        "            \"content\": {\n",
        "                \"categories\": {\"HasQuality\": {\"name\": \"has quality: does not say which it lack adequate evidence or it believes to be false.\"}, \"NonQuality\": {\"name\": \"no quality: says which it lacks evidence or it believes is false.\"}},\n",
        "                \"input\": \"radio\",\n",
        "            },\n",
        "        },\n",
        "        \"JOB_3\": {\n",
        "            \"mlTask\": \"CLASSIFICATION\",\n",
        "            \"required\": 1,\n",
        "            \"isChild\": False,\n",
        "            \"content\": {\n",
        "                \"categories\": {\"HasQuantity\": {\"name\": \"has quantity: is as informative as is required\"}, \"NoQuantity\": {\"name\": \"no  quantity: is more or less informative than is required\"}},\n",
        "                \"input\": \"radio\",\n",
        "            },\n",
        "        },\n",
        "        \"JOB_4\": {\n",
        "            \"mlTask\": \"CLASSIFICATION\",\n",
        "            \"required\": 1,\n",
        "            \"isChild\": False,\n",
        "            \"content\": {\n",
        "                \"categories\": {\"HasManner\": {\"name\": \"has manner: avoids vagueness, avoids ambiguty, do not play with words, is orderly\"}, \"NoManner\": {\"name\": \"no manner: is vague or ambiguous or plays with words or is not orderly\"}},\n",
        "                \"input\": \"radio\",\n",
        "            },\n",
        "        },\n",
        "        \"JOB_5\": {\n",
        "            \"mlTask\": \"CLASSIFICATION\",\n",
        "            \"required\": 1,\n",
        "            \"isChild\": False,\n",
        "            \"content\": {\n",
        "                \"categories\": {\"IsPolite\": {\"name\": \"is polite\"}, \"NotPolite\": {\"name\": \"is not polite\"}},\n",
        "                \"input\": \"radio\",\n",
        "            },\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "project = kili.create_project(\n",
        "    title=\"Conversation and Maxims\",\n",
        "    description=\"Project Description\",\n",
        "    input_type=\"TEXT\",\n",
        "    json_interface=interface,\n",
        ")\n",
        "project_id = project['id']\n",
        "print(\"Project ID: \", project_id)\n",
        "assets = kili.append_many_to_dataset(\n",
        "    project_id=project_id,\n",
        "    content_array = [('comment: ' + d['comment'] + '\\n' + 'reply: ' + d['reply']) for d in random_pairs[:100]],\n",
        "    external_id_array = [d['global_id'] for d in random_pairs[:100]]\n",
        ")\n",
        "# Project ID:  clp5pxaet1wdn086r7y3x0ngo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gz-OYtq6-iuo",
        "outputId": "4cf48f7b-27be-4fb9-c680-2f3eb6198c8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iqmyeuo: Hey, we launched a new music-scanning feature at Soundslice just a few days ago ([see here](https://www.soundslice.com/blog/226/pdf-and-photo-scanning-beta/)). It uses artificial intelligence to convert PDFs/images into editable sheet music.\n",
            "\n",
            "I'm happy to run a PDF through it for you if you'd like â€” just send me a DM!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[Node('/xsyewi', body=\"Has Anyone Tried Scanscore ?I've been trying to find a pdf scanner app or program that can scan sheet music and turn it into a **musicxml** format. I came across Scanscore, but I don't if it's any good. anyone in this sub try it? would you recommend it?\")]"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "\n",
        "# Define the sample size (e.g., 3 for a sample of 3 elements)\n",
        "sample_size = 1\n",
        "\n",
        "# Use random.sample to get a random sample of the specified size\n",
        "random_sample = random.sample(conversations_tree, sample_size)\n",
        "from anytree import PreOrderIter\n",
        "\n",
        "# Assuming 'root' is the root node of the conversation tree\n",
        "for node in PreOrderIter(random_sample[0]):\n",
        "    if node.parent is not None:\n",
        "        print(f\"{node.name}: {node.body}\")\n",
        "\n",
        "random_sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzvR0YnpSdWM",
        "outputId": "fbfba739-ce62-4c8d-f8c2-238995635e12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting kili\n",
            "  Downloading kili-2.148.1-py3-none-any.whl (288 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m288.7/288.7 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas<3.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from kili) (1.5.3)\n",
            "Requirement already satisfied: click<9.0.0,>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from kili) (8.1.7)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from kili) (2.31.0)\n",
            "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from kili) (0.9.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from kili) (8.2.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from kili) (4.66.1)\n",
            "Collecting typeguard<5,>=4 (from kili)\n",
            "  Downloading typeguard-4.1.5-py3-none-any.whl (34 kB)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from kili) (4.5.0)\n",
            "Requirement already satisfied: pyparsing<4.0.0,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from kili) (3.1.1)\n",
            "Requirement already satisfied: websocket-client<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from kili) (1.6.4)\n",
            "Requirement already satisfied: pyyaml<7.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from kili) (6.0.1)\n",
            "Requirement already satisfied: Pillow<11.0.0,>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from kili) (9.4.0)\n",
            "Collecting cuid<0.5,>=0.4 (from kili)\n",
            "  Downloading cuid-0.4.tar.gz (5.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from kili) (2.0.7)\n",
            "Collecting ffmpeg-python<0.3.0,>=0.2.0 (from kili)\n",
            "  Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
            "Collecting gql[requests,websockets]<4.0.0,>=3.5.0b5 (from kili)\n",
            "  Downloading gql-3.5.0b6-py2.py3-none-any.whl (72 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.6/72.6 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock<4.0.0,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from kili) (3.13.1)\n",
            "Collecting pyrate-limiter<3,>=2 (from kili)\n",
            "  Downloading pyrate_limiter-2.10.0-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from ffmpeg-python<0.3.0,>=0.2.0->kili) (0.18.3)\n",
            "Collecting graphql-core<3.4,>=3.3.0a3 (from gql[requests,websockets]<4.0.0,>=3.5.0b5->kili)\n",
            "  Downloading graphql_core-3.3.0a3-py3-none-any.whl (209 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m209.9/209.9 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: yarl<2.0,>=1.6 in /usr/local/lib/python3.10/dist-packages (from gql[requests,websockets]<4.0.0,>=3.5.0b5->kili) (1.9.2)\n",
            "Collecting backoff<3.0,>=1.11.1 (from gql[requests,websockets]<4.0.0,>=3.5.0b5->kili)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Collecting requests-toolbelt<2,>=1.0.0 (from gql[requests,websockets]<4.0.0,>=3.5.0b5->kili)\n",
            "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets<12,>=10 (from gql[requests,websockets]<4.0.0,>=3.5.0b5->kili)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0.0,>=1.0.0->kili) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0.0,>=1.0.0->kili) (2023.3.post1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0.0,>=1.0.0->kili) (1.23.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->kili) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->kili) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->kili) (2023.7.22)\n",
            "Collecting typing-extensions<5.0.0,>=4.1.0 (from kili)\n",
            "  Downloading typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas<3.0.0,>=1.0.0->kili) (1.16.0)\n",
            "Requirement already satisfied: multidict>=4.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.6->gql[requests,websockets]<4.0.0,>=3.5.0b5->kili) (6.0.4)\n",
            "Building wheels for collected packages: cuid\n",
            "  Building wheel for cuid (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cuid: filename=cuid-0.4-py2.py3-none-any.whl size=4712 sha256=b59c7136e6a0c35c44357a37f14df5f03d3a688b3754ee601d531698bd5923bd\n",
            "  Stored in directory: /root/.cache/pip/wheels/52/0a/dc/a0e28c435d5a74d9eef3d7c3cd147b96cb21e71e5ec7dcfdbe\n",
            "Successfully built cuid\n",
            "Installing collected packages: cuid, websockets, typing-extensions, pyrate-limiter, graphql-core, ffmpeg-python, backoff, typeguard, requests-toolbelt, gql, kili\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed backoff-2.2.1 cuid-0.4 ffmpeg-python-0.2.0 gql-3.5.0b6 graphql-core-3.3.0a3 kili-2.148.1 pyrate-limiter-2.10.0 requests-toolbelt-1.0.0 typeguard-4.1.5 typing-extensions-4.8.0 websockets-11.0.3\n"
          ]
        }
      ],
      "source": [
        "!pip install  kili"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udPeZAdjp0hd",
        "outputId": "2db17f27-bb07-4f36-95f7-00e31d48393f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parent: iqmum2s, Child: iqmunnz\n",
            "Parent: iqmum2s, Child: iqmuns8\n",
            "Parent: iqon7tc, Child: iqrmqmy\n",
            "xsy76f\n",
            "â”œâ”€â”€ iqmum2s\n",
            "â”‚   â”œâ”€â”€ iqmunnz\n",
            "â”‚   â””â”€â”€ iqmuns8\n",
            "â”œâ”€â”€ iqob6q0\n",
            "â””â”€â”€ iqon7tc\n",
            "    â””â”€â”€ iqrmqmy\n",
            "        â””â”€â”€ iqsvfoq\n"
          ]
        }
      ],
      "source": [
        "from anytree import Node, RenderTree\n",
        "\n",
        "# Create a list to store parent-child pairs\n",
        "parent_child_pairs = []\n",
        "\n",
        "# Access child nodes from the parent node and add the pairs to the list\n",
        "for parent in root.children:\n",
        "    for child in parent.children:\n",
        "        parent_child_pairs.append((parent.name, child.name))\n",
        "\n",
        "# Print the parent-child pairs\n",
        "for pair in parent_child_pairs:\n",
        "    print(f\"Parent: {pair[0]}, Child: {pair[1]}\")\n",
        "\n",
        "# Print the tree structure\n",
        "for pre, fill, node in RenderTree(root):\n",
        "    print(f\"{pre}{node.name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4brlEl0nruv",
        "outputId": "3d358491-66e0-46dd-aa34-c916afe67658"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data has been prepared for Kili labeling.\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "for comment in flat_comments_list:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJJSGUQj3m46"
      },
      "outputs": [],
      "source": [
        "formatted_data = []\n",
        "for comment in flat_comments_list:\n",
        "    formatted_comment = {\n",
        "        \"row_data\": comment['body'],\n",
        "        \"global_key\": comment['id'],\n",
        "        \"media_type\": \"TEXT\",\n",
        "        \"metadata_fields\": [],\n",
        "        \"attachments\": []\n",
        "    }\n",
        "    formatted_data.append(formatted_comment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LTQdXVen1Esj",
        "outputId": "b6f9e540-0d9b-491c-c361-9fa3cf5c0db5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data has been written to formatted_comments.json\n"
          ]
        }
      ],
      "source": [
        "with open(base_path +\"flat_comments_list.json\", \"w\") as json_file:\n",
        "    json.dump(flat_comments_list, json_file, indent=2)\n",
        "\n",
        "print(\"Data has been written to formatted_comments.json\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T62FjApSor6r"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyObQ09sYzHQ/yE4JJo+Z8c/",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}