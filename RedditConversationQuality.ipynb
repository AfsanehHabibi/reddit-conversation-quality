{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AfsanehHabibi/reddit-conversation-quality/blob/main/RedditConversationQuality.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GG5cUM4Rsu2T"
      },
      "source": [
        "# Scraping Reddit Data  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HyWb03HpZBL0"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFO8RGB2Noqw"
      },
      "source": [
        "#Pushshift API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdTp7_d_riAo"
      },
      "outputs": [],
      "source": [
        "base_path = \"/content/drive/MyDrive/University/RedditData/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiA0pL0yj5b_"
      },
      "source": [
        "#Create sampling timestamps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "koEKdls8CXcH"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import datetime\n",
        "def create_timestamps(seed, num_values, period_length, year, month, day):\n",
        "    random.seed(seed)\n",
        "    start_date = datetime.datetime(year, month, day)\n",
        "    minutes_after = set()\n",
        "    scale = 24*60 / period_length\n",
        "    while len(minutes_after) < num_values:\n",
        "        day_after = random.randint(0, 7)\n",
        "        minutes_after.add(day_after*24*60 + random.randint(0, scale)*period_length)\n",
        "    timestamps = []\n",
        "    for minute in minutes_after:\n",
        "        timestamp = start_date + datetime.timedelta(minutes=minute)\n",
        "        end_timestamp = timestamp + datetime.timedelta(minutes=period_length)\n",
        "        timestamps.append((int(timestamp.timestamp()), int(end_timestamp.timestamp())))\n",
        "    return timestamps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6PIgi-g5mwj9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "def write_timestamps_to_file(seed, num_values, period_length, year, month, day):\n",
        "    # Create the timestamps using the provided function\n",
        "    timestamps = create_timestamps(seed, num_values, period_length, year, month, day)\n",
        "    \n",
        "    # Generate the file name based on the input arguments\n",
        "    file_name = f\"{base_path}timestamps_seed{seed}_num{num_values}_period{period_length}_date{year}-{month}-{day}.txt\"\n",
        "    \n",
        "    # Write the timestamps to the file\n",
        "    if os.path.exists(file_name):\n",
        "        print(f\"File already exists: {file_name}\")\n",
        "        return\n",
        "    with open(file_name, \"w\") as f:\n",
        "        for start_time, end_time in timestamps:\n",
        "            f.write(f\"{start_time},{end_time},0\\n\")\n",
        "    \n",
        "    # Print the file path for reference\n",
        "    print(f\"Timestamps file saved to {os.getcwd()}/{file_name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RHUYCBqon_4_"
      },
      "outputs": [],
      "source": [
        "write_timestamps_to_file(seed=2, num_values=20, period_length=10, year=2022, month=10, day=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTIp2yl2DAtZ"
      },
      "source": [
        "##Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ps5Cvyv-wL92"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def filter_json_objects(data, keys):\n",
        "    filtered_data = []\n",
        "    for item in data:\n",
        "        filtered_item = {}\n",
        "        for key in keys:\n",
        "            if key in item:\n",
        "                filtered_item[key] = item[key]\n",
        "        filtered_data.append(filtered_item)\n",
        "    return filtered_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6AoO4Y0BaXJD"
      },
      "outputs": [],
      "source": [
        "def write_last_timestamp(last_timestamp, entry, id):\n",
        "  with open(f\"{base_path}last_timestamp_{entry}_{id}.txt\", \"w\") as f:\n",
        "    f.write(str(last_timestamp))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zmavg4ISbQYg"
      },
      "outputs": [],
      "source": [
        "def read_last_timestamp(default, entry, id):\n",
        "  try:\n",
        "    with open(f\"{base_path}last_timestamp_{entry}_{id}.txt\", \"r\") as f:\n",
        "        return int(f.read())\n",
        "  except FileNotFoundError:\n",
        "    return default"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OETKJ4aicc1y"
      },
      "outputs": [],
      "source": [
        "def write_data_to_file(data, entry, id):\n",
        "  with open(f\"{base_path}{entry}s_{id}.json\", \"a\") as f:\n",
        "    for element in data:\n",
        "        json.dump(element, f)\n",
        "        f.write(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mmmiWye618l5"
      },
      "outputs": [],
      "source": [
        "def create_fields_filter_in_url(fields):\n",
        "    \"\"\"\n",
        "    Creates a filter query parameter for the API endpoint based on the specified fields.\n",
        "\n",
        "    Parameters:\n",
        "    - fields (list): a list of field names to include in the query results. \n",
        "      If the list has only one element \"all\", all fields are included.\n",
        "\n",
        "    Returns:\n",
        "    - filter_query (str): the constructed query parameter string to append to the API endpoint URL.\n",
        "    \"\"\"\n",
        "    if len(fields) == 1 and fields[0] == \"all\":\n",
        "        return \"\"\n",
        "    else:\n",
        "        filter_query = \"&filter=\" + ','.join(fields)\n",
        "        return filter_query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oAKT7XO8sX2F"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import datetime\n",
        "import json\n",
        "import time\n",
        "\n",
        "def extract_date_based_data_from_reddit(id, entry_type, keys, start_date, end_date, step):\n",
        "  # type can be either submission or comment\n",
        "  # keys are fields from submission or comment that we want to save\n",
        "  # start date and end date shows the date interval which data is collected\n",
        "\n",
        "\n",
        "  # read the last UTC timestamp from file\n",
        "  last_timestamp = read_last_timestamp(int(end_date.timestamp()), entry_type, id)\n",
        "\n",
        "  if last_timestamp == int(start_date.timestamp()):\n",
        "    return\n",
        "  # define the API endpoint\n",
        "  filter_query = create_fields_filter_in_url(keys)\n",
        "  const_url = f\"https://api.pushshift.io/reddit/search/{entry_type}/?size=500{filter_query}&sort=created_utc&\"\n",
        "  url_template = const_url + \"after={}&before={}\"\n",
        "  # get the set of enteries using pagination\n",
        "  while True:\n",
        "    new_url = url_template.format(\n",
        "        max(last_timestamp-step,int(start_date.timestamp())),\n",
        "         last_timestamp)\n",
        "    try:\n",
        "        new_response = requests.get(new_url)\n",
        "        enteries = new_response.json()[\"data\"]\n",
        "        write_data_to_file(enteries, entry_type, id)\n",
        "        # save the UTC timestamp of the last entry to file\n",
        "        print(len(enteries))\n",
        "        if len(enteries) == 0:\n",
        "          last_timestamp -= step\n",
        "          continue\n",
        "        last_entry = enteries[-1]\n",
        "        last_timestamp = last_entry[\"created_utc\"]\n",
        "        last_entry_date = datetime.datetime.fromtimestamp(last_timestamp)\n",
        "        print(last_entry_date)\n",
        "        write_last_timestamp(last_timestamp, entry_type, id)\n",
        "        if last_timestamp == int(start_date.timestamp()):\n",
        "          break\n",
        "    except (json.JSONDecodeError, requests.exceptions.HTTPError) as e:\n",
        "        if new_response.status_code == 429 :\n",
        "            retry_after = 30#int(new_response.headers.get(\"Retry-After\"))\n",
        "            print(f\"Got HTTP error 429, waiting {retry_after} seconds and retrying...\")\n",
        "            time.sleep(retry_after)\n",
        "            continue\n",
        "        elif new_response.status_code == 524:\n",
        "            retry_after = 60\n",
        "            print(f\"Got HTTP error 524, waiting {retry_after} seconds and retrying...\")\n",
        "            time.sleep(retry_after)\n",
        "            continue\n",
        "        else:\n",
        "            raise e"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHko-ax38x9J"
      },
      "outputs": [],
      "source": [
        "def update_timestamps_file(file_path, starting_timestamp, status):\n",
        "    # Read the contents of the file\n",
        "    with open(file_path, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "    \n",
        "    # Find the line with the starting timestamp\n",
        "    for i, line in enumerate(lines):\n",
        "        start_time, end_time, old_status = line.strip().split(',')\n",
        "        if int(start_time) == starting_timestamp:\n",
        "            # Update the status to the new value\n",
        "            lines[i] = f\"{start_time},{end_time},{status}\\n\"\n",
        "            break\n",
        "    \n",
        "    # Write the updated contents back to the file\n",
        "    with open(file_path, 'w') as f:\n",
        "        f.writelines(lines)\n",
        "        f.flush()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9wSmlZgrJrJ"
      },
      "outputs": [],
      "source": [
        "def extract(seed, num_values, period_length, year, month, day, entry_type, keys):\n",
        "    file_path = f\"{base_path}timestamps_seed{seed}_num{num_values}_period{period_length}_date{year}-{month}-{day}.txt\"\n",
        "    \n",
        "    while True:\n",
        "        all_done = True\n",
        "        with open(file_path, \"r\") as f:\n",
        "            for line in f:\n",
        "                print(line)\n",
        "                start, end, done_str = line.strip().split(\",\")\n",
        "                done = int(done_str) == 1\n",
        "                if not done:\n",
        "                    print(int(start), int(end))\n",
        "                    timestamp = (int(start), int(end))\n",
        "                    all_done = False\n",
        "                    break\n",
        "        if all_done:\n",
        "          break\n",
        "        start_date = datetime.datetime.fromtimestamp(timestamp[0])\n",
        "        end_date = datetime.datetime.fromtimestamp(timestamp[1])\n",
        "        extract_date_based_data_from_reddit(timestamp[0], entry_type, keys, start_date, end_date, 10)\n",
        "        update_timestamps_file(file_path, timestamp[0], 1)\n",
        "    return timestamps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KEfn1a082eIZ"
      },
      "outputs": [],
      "source": [
        "extract(seed=2, num_values=20, period_length=10, year=2022, month=10, day=1, entry_type=\"submission\", keys=[\"all\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jM7l0XFdD7Fp"
      },
      "outputs": [],
      "source": [
        "with open(f'{base_path}submissions_1664946000.json', 'r') as f:\n",
        "    num_lines = len(f.readlines())\n",
        "    print(f\"The file contains {num_lines} lines.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoSWqWGwmBtx"
      },
      "source": [
        "##Write submmisons to file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UN85zkHru-23"
      },
      "outputs": [],
      "source": [
        "# define the date range\n",
        "start_date = datetime.datetime(2022, 3, 1)\n",
        "end_date = datetime.datetime(2022, 3, 31)\n",
        "keys = [\"id\",\"subreddit\",\"selftext\",\"title\",\"quarantine\",\"is_original_content\",\"is_meta\",\"is_created_from_ads_ui\",\"author_premium\",\"is_self\",\"subreddit_type\",\"allow_live_comments\",\"is_crosspostable\",\"over_18\",\"removed_by\",\"distinguished\",\"subreddit_id\",\"author\",\"discussion_type\",\"num_comments\",\"whitelist_status\",\"subreddit_subscribers\",\"created_utc\",\"retrieved_utc\",\"updated_utc\",\"media_metadata\"]\n",
        "\n",
        "extract_date_based_data_from_reddit(int(start_date.timestamp()), \"submission\", [\"all\"], start_date, end_date, 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ltXSj81UffF"
      },
      "source": [
        "##Write sample submission fields to file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IoYOExFgUcr-"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "# define the API endpoint with the sample submission ID\n",
        "submission_id = \"1273r9g\"\n",
        "url = f\"https://api.pushshift.io/reddit/submission/search/?ids={submission_id}\"\n",
        "\n",
        "# make the API request and get the submission\n",
        "response = requests.get(url)\n",
        "submission_list = response.json()[\"data\"]\n",
        "\n",
        "# get the first submission from the list\n",
        "submission = submission_list[0]\n",
        "\n",
        "# write the submission keys to a file\n",
        "with open(f\"{base_path}submission_keys.txt\", \"w\") as f:\n",
        "    for key in submission.keys():\n",
        "        f.write(key + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuQjrPqCZJ6m"
      },
      "source": [
        "##Write comments to file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQbtxXD-mSp3"
      },
      "source": [
        "###With Pushshift date search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8aA7VAwkDhpl"
      },
      "outputs": [],
      "source": [
        "# define the date range\n",
        "start_date = datetime.datetime(2023, 3, 1)\n",
        "end_date = datetime.datetime(2023, 3, 31)\n",
        "keys = ['created_utc', 'id']\n",
        "\n",
        "extract_date_based_data_from_reddit(\"comment\", keys, start_date, end_date, 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dfcumy29-t78"
      },
      "source": [
        "##Write sample submission fields to file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OhgCTXsN-z5l"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "# define the API endpoint with the sample comment ID\n",
        "comment_id = \"jece0zo\"\n",
        "url = f\"https://api.pushshift.io/reddit/comment/search/?ids={comment_id}\"\n",
        "\n",
        "# make the API request and get the submission\n",
        "response = requests.get(url)\n",
        "comment_list = response.json()[\"data\"]\n",
        "\n",
        "# get the first submission from the list\n",
        "comment = comment_list[0]\n",
        "\n",
        "# write the submission keys to a file\n",
        "with open(f\"{base_path}comment_keys.txt\", \"w\") as f:\n",
        "    for key in comment.keys():\n",
        "        f.write(key + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Q52ZikM3xzK"
      },
      "outputs": [],
      "source": [
        "def test_create_fields_filter_in_url():\n",
        "    # Test case 1: when fields is ['all']\n",
        "    fields = ['all']\n",
        "    expected_output = ''\n",
        "    assert create_fields_filter_in_url(fields) == expected_output\n",
        "    \n",
        "    # Test case 2: when fields is empty\n",
        "    fields = []\n",
        "    expected_output = '&filter='\n",
        "    assert create_fields_filter_in_url(fields) == expected_output\n",
        "    \n",
        "    # Test case 3: when fields contains multiple values\n",
        "    fields = ['field1', 'field2', 'field3']\n",
        "    expected_output = '&filter=field1,field2,field3'\n",
        "    assert create_fields_filter_in_url(fields) == expected_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EE8oQLeLSx5k"
      },
      "outputs": [],
      "source": [
        "test_create_fields_filter_in_url()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DT0Cy0SSf5f6"
      },
      "outputs": [],
      "source": [
        "def test_create_timestamps():\n",
        "    # Create timestamps with 5-minute periods for 10 values starting from 2022-01-01\n",
        "    timestamps = create_timestamps(seed=123, num_values=10, period_length=5, year=2022, month=1, day=1)\n",
        "\n",
        "    # Check that there are no overlaps between periods\n",
        "    for i in range(len(timestamps)):\n",
        "        for j in range(i+1, len(timestamps)):\n",
        "            start_i, end_i = timestamps[i]\n",
        "            start_j, end_j = timestamps[j]\n",
        "            assert end_i <= start_j or end_j <= start_i, \"Overlapping periods\"\n",
        "\n",
        "    # Check that all periods start after or at the starting day\n",
        "    start_date = datetime.datetime(2022, 1, 1)\n",
        "    for start, end in timestamps:\n",
        "        assert start >= start_date.timestamp(), \"Period starting before start day\"\n",
        "\n",
        "    # Check that all periods end at most 7 days after the starting day\n",
        "    end_date = start_date + datetime.timedelta(days=7)\n",
        "    for start, end in timestamps:\n",
        "        assert end <= end_date.timestamp(), \"Period ending more than 7 days after start day\""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}