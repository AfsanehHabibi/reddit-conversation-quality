{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AfsanehHabibi/reddit-conversation-quality/blob/main/RedditConversationQuality.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GG5cUM4Rsu2T"
      },
      "source": [
        "# Scraping Reddit Data  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "HyWb03HpZBL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Pushshift API"
      ],
      "metadata": {
        "id": "EFO8RGB2Noqw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_path = \"/content/drive/MyDrive/University/RedditData/\""
      ],
      "metadata": {
        "id": "bdTp7_d_riAo"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Functions"
      ],
      "metadata": {
        "id": "CTIp2yl2DAtZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def filter_json_objects(data, keys):\n",
        "    filtered_data = []\n",
        "    for item in data:\n",
        "        filtered_item = {}\n",
        "        for key in keys:\n",
        "            if key in item:\n",
        "                filtered_item[key] = item[key]\n",
        "        filtered_data.append(filtered_item)\n",
        "    return filtered_data"
      ],
      "metadata": {
        "id": "ps5Cvyv-wL92"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def write_last_timestamp(last_timestamp, entry):\n",
        "  with open(f\"{base_path}last_timestamp_{entry}.txt\", \"w\") as f:\n",
        "    f.write(str(last_timestamp))"
      ],
      "metadata": {
        "id": "6AoO4Y0BaXJD"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_last_timestamp(default, entry):\n",
        "  try:\n",
        "    with open(f\"{base_path}last_timestamp_{entry}.txt\", \"r\") as f:\n",
        "        return int(f.read())\n",
        "  except FileNotFoundError:\n",
        "    return default"
      ],
      "metadata": {
        "id": "zmavg4ISbQYg"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def write_data_to_file(data, entry):\n",
        "  with open(f\"{base_path}{entry}s.json\", \"a\") as f:\n",
        "    for element in data:\n",
        "        json.dump(element, f)\n",
        "        f.write(\"\\n\")"
      ],
      "metadata": {
        "id": "OETKJ4aicc1y"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import datetime\n",
        "import json\n",
        "import time\n",
        "\n",
        "def extract_date_based_data_from_reddit(entry_type, keys, start_date, end_date, step):\n",
        "  # type can be either submission or comment\n",
        "  # keys are fields from submission or comment that we want to save\n",
        "  # start date and end date shows the date interval which data is collected\n",
        "\n",
        "\n",
        "  # read the last UTC timestamp from file\n",
        "  last_timestamp = read_last_timestamp(int(end_date.timestamp()), entry_type)\n",
        "\n",
        "  # define the API endpoint\n",
        "  const_url = f\"https://api.pushshift.io/reddit/search/{entry_type}/?size=500&filter={','.join(keys)}&sort=created_utc&\"\n",
        "  url_template = const_url + \"after={}&before={}\"\n",
        "  # get the set of enteries using pagination\n",
        "  while True:\n",
        "    new_url = url_template.format(\n",
        "        max(last_timestamp-step,int(start_date.timestamp())),\n",
        "         last_timestamp)\n",
        "    try:\n",
        "        new_response = requests.get(new_url)\n",
        "        enteries = new_response.json()[\"data\"]\n",
        "        write_data_to_file(enteries, entry_type)\n",
        "        # save the UTC timestamp of the last entry to file\n",
        "        print(len(enteries))\n",
        "        if len(enteries) == 0:\n",
        "          last_timestamp -= step\n",
        "          continue\n",
        "        last_entry = enteries[-1]\n",
        "        last_timestamp = last_entry[\"created_utc\"]\n",
        "        last_entry_date = datetime.datetime.fromtimestamp(last_timestamp)\n",
        "        print(last_entry_date)\n",
        "        write_last_timestamp(last_timestamp, entry_type)\n",
        "        if last_timestamp == int(start_date.timestamp()):\n",
        "          break\n",
        "    except json.JSONDecodeError or requests.exceptions.HTTPError:\n",
        "        if new_response.status_code == 429:\n",
        "            retry_after = 30#int(new_response.headers.get(\"Retry-After\"))\n",
        "            print(f\"Got HTTP error 429, waiting {retry_after} seconds and retrying...\")\n",
        "            time.sleep(retry_after)\n",
        "            continue\n",
        "        else:\n",
        "            raise e"
      ],
      "metadata": {
        "id": "oAKT7XO8sX2F"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Write submmisons to file"
      ],
      "metadata": {
        "id": "hoSWqWGwmBtx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define the date range\n",
        "start_date = datetime.datetime(2023, 3, 1)\n",
        "end_date = datetime.datetime(2023, 3, 31)\n",
        "keys = ['created_utc', 'id']\n",
        "\n",
        "extract_date_based_data_from_reddit(\"submission\", keys, start_date, end_date, 10)"
      ],
      "metadata": {
        "id": "UN85zkHru-23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Write sample submission fields to file"
      ],
      "metadata": {
        "id": "1ltXSj81UffF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "# define the API endpoint with the sample submission ID\n",
        "submission_id = \"1273r9g\"\n",
        "url = f\"https://api.pushshift.io/reddit/submission/search/?ids={submission_id}\"\n",
        "\n",
        "# make the API request and get the submission\n",
        "response = requests.get(url)\n",
        "submission_list = response.json()[\"data\"]\n",
        "\n",
        "# get the first submission from the list\n",
        "submission = submission_list[0]\n",
        "\n",
        "# write the submission keys to a file\n",
        "with open(\"submission_keys.txt\", \"w\") as f:\n",
        "    for key in submission.keys():\n",
        "        f.write(key + \"\\n\")"
      ],
      "metadata": {
        "id": "IoYOExFgUcr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Write comments to file"
      ],
      "metadata": {
        "id": "TuQjrPqCZJ6m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###With Pushshift date search"
      ],
      "metadata": {
        "id": "SQbtxXD-mSp3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define the date range\n",
        "start_date = datetime.datetime(2023, 3, 1)\n",
        "end_date = datetime.datetime(2023, 3, 31)\n",
        "keys = ['created_utc', 'id']\n",
        "\n",
        "extract_date_based_data_from_reddit(\"comment\", keys, start_date, end_date, 2)"
      ],
      "metadata": {
        "id": "8aA7VAwkDhpl"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "28Ala95hy1Ic",
        "Qfb-qU2jXRuy"
      ],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}